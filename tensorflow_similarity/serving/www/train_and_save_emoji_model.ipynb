{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "from cv2 import imread\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten, Input,\n",
    "                                     MaxPooling2D, Reshape, UpSampling2D, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_similarity.api.engine.simhash import SimHash\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in emoji dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"emoji_dataset\"\n",
    "data_dir = pathlib.Path(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_paths = list(data_dir.glob('*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=72x72 at 0x7F5B1B2F8B10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display.display(Image.open(str(all_images_paths[95])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(all_images_paths)\n",
    "x_targets = []\n",
    "y_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in all_images_paths:\n",
    "    path_str = str(image_path)\n",
    "    y_target = path_str.split(\"__\")[0].split(\"/\")[1]\n",
    "    x_target = imread(path_str)\n",
    "    x_target = tf.image.rgb_to_grayscale(x_target).numpy()\n",
    "    x_target = np.max(x_target) - x_target\n",
    "    x_target = x_target / np.max(x_target)\n",
    "    x_targets.append(x_target)\n",
    "    y_targets.append(y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon_size = 32 # original size 72\n",
    "num_augment = 50\n",
    "num_shots = 4 # at most 5\n",
    "num_unseen_classes = 20 # original classes ~1700 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_targets = np.asarray(x_targets)\n",
    "y_targets = np.asarray(y_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_number(y_targets):\n",
    "    uniques = np.unique(y_targets)\n",
    "    unique_dicts = dict()\n",
    "    for i, label in enumerate(uniques):\n",
    "        unique_dicts[label] = i\n",
    "    result = np.zeros(y_targets.shape, dtype=int)\n",
    "    for i, target in enumerate(y_targets):\n",
    "        result[i] = unique_dicts[target]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets = label_to_number(y_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Augment Emoji Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_learning_split(x_data, y_data, num_unseen_classes=5, num_shots=1, new_size=32, num_augment=20):\n",
    "    \n",
    "    x_data = tf.image.resize_with_pad(x_data, new_size, new_size)\n",
    "    datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest') # FIXME: vertical_flip = True\n",
    "\n",
    "    datagen.fit(x_data)\n",
    "    \n",
    "    if num_augment > 0:\n",
    "        print(\"performing data augmentation\")\n",
    "        batch_size = 64 * 4\n",
    "        new_x_train = np.zeros((len(x_data) * num_augment, new_size, new_size, 1))\n",
    "        new_y_train = np.empty(len(new_x_train), dtype=object)\n",
    "\n",
    "        num_steps = int(len(new_x_train) / batch_size)\n",
    "\n",
    "        image_generator = datagen.flow(x=x_data, y=y_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        # pbar that we want to update\n",
    "        pbar = tqdm(total=len(new_x_train))\n",
    "        idx = 0\n",
    "        while idx < len(new_x_train):\n",
    "            x_train_batch, y_train_batch = next(image_generator)\n",
    "\n",
    "            batch_size = len(x_train_batch)\n",
    "            batch_size = min(batch_size, len(new_x_train) - idx)\n",
    "            new_x_train[idx:idx+batch_size] = x_train_batch\n",
    "            new_y_train[idx:idx+batch_size] = y_train_batch\n",
    "\n",
    "            idx += batch_size\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "        x_data = new_x_train\n",
    "        y_data = new_y_train\n",
    "    \n",
    "    unique_classes = np.unique(y_data)\n",
    "    test_classes = np.random.choice(\n",
    "        unique_classes,\n",
    "        num_unseen_classes,\n",
    "        replace=False)\n",
    "    \n",
    "    # classes that are not selected are available for training\n",
    "    available_train_classes = np.setdiff1d(\n",
    "        unique_classes, test_classes, assume_unique=True)\n",
    "    \n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    x_target = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    y_target = []\n",
    "    \n",
    "    seen = defaultdict(int)\n",
    "\n",
    "    for image, label in zip(x_data, y_data):\n",
    "        if label in test_classes:\n",
    "            if seen[label] < num_shots:\n",
    "                x_target.append(image)\n",
    "                y_target.append(label)\n",
    "                seen[label] += 1\n",
    "            else:\n",
    "                x_test.append(image)\n",
    "                y_test.append(label)\n",
    "        elif label in available_train_classes:\n",
    "            x_train.append(image)\n",
    "            y_train.append(label)\n",
    "\n",
    "    x_train = {\"image\": np.array(x_train)}\n",
    "    y_train = np.array(y_train)\n",
    "    x_test = {\"image\": np.array(x_test)}\n",
    "    y_test = np.array(y_test)\n",
    "    x_target = {\"image\": np.array(x_target)}\n",
    "    y_target = np.array(y_target)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), (x_target, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing data augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b454ea091b4f0b9b0f5007818a6639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=394000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), (x_target, y_target) = one_shot_learning_split(x_targets, y_targets, num_unseen_classes, num_shots, icon_size, num_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = x_train[\"image\"]\n",
    "x_train_data = (np.max(x_train_data) - x_train_data) / np.max(x_train_data)\n",
    "x_train[\"image\"] = x_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"A simple tower model for mnist dataset.\n",
    "    \n",
    "    Returns:\n",
    "        model: A tensorflow model.\n",
    "    \"\"\"\n",
    "    \n",
    "    i = Input(shape=(icon_size, icon_size, 1), name=\"image\")\n",
    "    o = Conv2D(32, kernel_size=(5, 5), activation='relu')(i)\n",
    "    o = Conv2D(32, kernel_size=(5, 5), activation='relu')(o)\n",
    "    o = Conv2D(64, (3, 3), padding='same', activation='relu')(o)\n",
    "    o = Conv2D(64, (3, 3), padding='same', activation='relu')(o)\n",
    "\n",
    "    o = tf.keras.layers.GlobalMaxPool2D()(o)\n",
    "    o = Dense(64)(o)\n",
    "    o = Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name=\"l2_norm\")(o)\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"triplet_loss\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_image (InputLayer)       [(None, 32, 32, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "anchor_idx (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neg_image (InputLayer)          [(None, 32, 32, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neg_idx (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_image (InputLayer)          [(None, 32, 32, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_idx (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 [(None, 1), (None, 1 86048       anchor_image[0][0]               \n",
      "                                                                 anchor_idx[0][0]                 \n",
      "                                                                 neg_image[0][0]                  \n",
      "                                                                 neg_idx[0][0]                    \n",
      "                                                                 pos_image[0][0]                  \n",
      "                                                                 pos_idx[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "anchor_idx_out (Rename)         (None, 1)            0           model_9[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "neg_idx_out (Rename)            (None, 1)            0           model_9[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "pos_idx_out (Rename)            (None, 1)            0           model_9[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "triplets (Rename)               (None, 1)            0           model_9[1][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "meta_is_hard (Rename)           (None, 1)            0           model_9[1][4]                    \n",
      "__________________________________________________________________________________________________\n",
      "hard_tuple (Rename)             (None, 1)            0           model_9[1][5]                    \n",
      "==================================================================================================\n",
      "Total params: 86,048\n",
      "Trainable params: 86,048\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "1540/1540 [==============================] - 59s 38ms/step - loss: 0.0593 - triplets_loss: 0.0593 - hard_tuple_fraction: 0.3203\n",
      "Epoch 2/20\n",
      "Epoch 1/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0488 - triplets_loss: 0.0488 - hard_tuple_fraction: 0.2689\n",
      "Epoch 3/20\n",
      "1540/1540 [==============================] - 58s 37ms/step - loss: 0.0437 - triplets_loss: 0.0437 - hard_tuple_fraction: 0.2435\n",
      "Epoch 4/20\n",
      "1540/1540 [==============================] - 58s 38ms/step - loss: 0.0405 - triplets_loss: 0.0405 - hard_tuple_fraction: 0.2266\n",
      "Epoch 5/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0388 - triplets_loss: 0.0388 - hard_tuple_fraction: 0.2178\n",
      "Epoch 6/20\n",
      "1540/1540 [==============================] - 58s 37ms/step - loss: 0.0372 - triplets_loss: 0.0372 - hard_tuple_fraction: 0.2111\n",
      "Epoch 7/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0355 - triplets_loss: 0.0355 - hard_tuple_fraction: 0.2019\n",
      "Epoch 8/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0346 - triplets_loss: 0.0346 - hard_tuple_fraction: 0.1979\n",
      "Epoch 9/20\n",
      "1540/1540 [==============================] - 58s 38ms/step - loss: 0.0332 - triplets_loss: 0.0332 - hard_tuple_fraction: 0.1916\n",
      "Epoch 10/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0323 - triplets_loss: 0.0323 - hard_tuple_fraction: 0.1866\n",
      "Epoch 11/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0319 - triplets_loss: 0.0319 - hard_tuple_fraction: 0.1843\n",
      "Epoch 12/20\n",
      "1539/1540 [============================>.] - ETA: 0s - loss: 0.0310 - triplets_loss: 0.0310 - hard_tuple_fraction: 0.1801\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0310 - triplets_loss: 0.0310 - hard_tuple_fraction: 0.1802\n",
      "Epoch 13/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0303 - triplets_loss: 0.0303 - hard_tuple_fraction: 0.1772\n",
      "Epoch 14/20\n",
      "1540/1540 [==============================] - 58s 38ms/step - loss: 0.0300 - triplets_loss: 0.0300 - hard_tuple_fraction: 0.1754\n",
      "Epoch 15/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0292 - triplets_loss: 0.0292 - hard_tuple_fraction: 0.1722\n",
      "Epoch 16/20\n",
      "1540/1540 [==============================] - 58s 37ms/step - loss: 0.0289 - triplets_loss: 0.0289 - hard_tuple_fraction: 0.1696\n",
      "Epoch 17/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0283 - triplets_loss: 0.0283 - hard_tuple_fraction: 0.1670\n",
      "Epoch 18/20\n",
      "1540/1540 [==============================] - 58s 37ms/step - loss: 0.0281 - triplets_loss: 0.0281 - hard_tuple_fraction: 0.1655\n",
      "Epoch 19/20\n",
      "1539/1540 [============================>.] - ETA: 0s - loss: 0.0276 - triplets_loss: 0.0276 - hard_tuple_fraction: 0.1638Epoch 19/20\n",
      "1540/1540 [==============================] - 58s 38ms/step - loss: 0.0276 - triplets_loss: 0.0276 - hard_tuple_fraction: 0.1638\n",
      "Epoch 20/20\n",
      "1540/1540 [==============================] - 57s 37ms/step - loss: 0.0272 - triplets_loss: 0.0272 - hard_tuple_fraction: 0.1618\n",
      "Epoch 20/20"
     ]
    }
   ],
   "source": [
    "moirai = SimHash(\n",
    "    model,\n",
    "    strategy=\"triplet_loss\",\n",
    "    batch_size=256,\n",
    "    optimizer=Adam(lr=.001))\n",
    "\n",
    "moirai.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20)\n",
    "\n",
    "moirai.save('new_smily_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
