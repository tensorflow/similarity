# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""This file contains the Result object for Benchmark Tensorflow Similarity
models."""

import json
import os
from collections import defaultdict
from google.cloud import bigquery
import numpy as np


class Result(object):
    """The Result class that stores the benchmark information.

    This class has the following fields (BigQuery terminology)

    model_information (RECORD)
    model_information.embedding_size (INTEGER)
    model_information.tower_model_architecture (STRING)
    model_information.auxiliary_task (STRING)
    model_information.similarity_loss (STRING)

    data_information (RECORD)
    data_information.num_unseen_classes (INTEGER)
    data_information.num_targets (INTEGER)
    data_information.training_classes_percentage (FLOAT)
    data_information.num_training_samples (INTEGER)

    training_information (RECORD)
    training_information.num_epochs (INTEGER)
    training_information.inference_time (FLOAT)

    epoch_level_metrics (RECORD)
    epoch_level_metrics.accuracy (REPEATED: FLOAT)
    epoch_level_metrics.v_measure (REPEATED: FLOAT)
    epoch_level_metrics.completeness (REPEATED: FLOAT)
    epoch_level_metrics.homogenity (REPEATED: FLOAT)
    epoch_level_metrics.silhouette_score (REPEATED: FLOAT)

    best_model_metrics (RECORD)
    best_model_metrics.accuracy (FLOAT)
    best_model_metrics.epoch (INTEGER)
    best_model_metrics.target_embeddings (REPEATED: FLOAT)
    best_model_metrics.test_embeddings (REPEATED: FLOAT)
    best_model_metrics.target_labels (REPEATED: INTEGER)
    best_model_metrics.test_labels (REPEATED: INTEGER)

    The model, data, and training information are all provided by the users
    (with the exception of training_information.inference_time), the metrics
    are computed and stored throughout training process.

    Args:
        model_information (dict, optional): model information contains
            information such as tower model architecture,
            embedding size, similarity loss, auxsiliary task, etc.
            Defaults to {}.
        data_information (dict, optional): data information contains
            information such as dataset name, number of unseen classes,
            ratio of seen classes, etc. Defaults to {}.
        num_epochs (int, optional): Number of training epochs.
            Defaults to 0.
    """

    def __init__(self, model_information={}, data_information={}):

        self.model_information = model_information
        self.data_information = data_information

        self.training_information = dict()
        self.training_information["num_epochs"] = 0
        # inference time will be updated at the end of training
        self.training_information["inference_time"] = 0.0

        # metrics that we want to record after the end of each epochs
        # all of them will initialized with defaultdict(list) so we don't
        # need to specify the types here
        epoch_recording_metrics = [
            "accuracy",
            "v_measure",
            "completeness",
            "homogeneity",
            "silhouette_score"]

        # dictionary that points to all epoch level metrics
        # those would be computed and stored at the end of each epochs
        # Each of those metrics stores different n_ways_k_shot metric throughout
        # the training process.
        # Example: {"5_ways_1_shot": [0.23, 0.95],
        #           "5_ways_5_shots": [0.12, 0.8]}.
        self.epoch_level_metrics = dict()
        for metric_name in epoch_recording_metrics:
            self.epoch_level_metrics[metric_name] = defaultdict(list)

        # Metrics that we want to record at the best model (currently proxy to
        # when the model has the best accuracy)
        # Accuracy store the best n_ways_k_shot accuracy throughout the
        # training process.
        # Epoch store the number of epochs to reach the best accuracy
        # for n_ways_k_shots
        # Embeddings store flattened float arrays of target and test embeddings
        # generated by the model at the best accuracy for each n-ways-k-shots
        # the embedding size is stored in model information so users can
        # retrieve the embeddings.
        # Labels store the list of labels for the embeddings.
        best_recording_metrics = [
            ("accuracy", float),
            ("epoch", int),
            ("target_embeddings", list),
            ("test_embeddings", list),
            ("target_labels", list),
            ("test_labels", list)]

        self.best_model_metrics = dict()
        for metric_name, metric_type in best_recording_metrics:
            self.best_model_metrics[metric_name] = defaultdict(metric_type)

    def get_num_unseen_classes(self):
        return self.data_information["num_unseen_classes"]

    def get_num_targets(self):
        return self.data_information["num_targets"]

    def get_num_epochs(self):
        return self.training_information["num_epochs"]

    def get_num_training_samples(self):
        return self.data_information["num_training_samples"]

    def get_training_classes_percentage(self):
        return self.data_information["training_classes_percentage"]

    def get_num_training_classes(self):
        return self.data_information["num_seen_classes"]

    def get_best_accuracy(self, N, k):
        key = self._make_key(N, k)
        return self.best_model_metrics["accuracy"][key]

    def get_best_epoch(self, N, k):
        key = self._make_key(N, k)
        return self.best_model_metrics["epoch"][key]

    def get_available_metrics(self):
        return list(self.epoch_level_metrics.keys())

    def get_metric(self, metric_name, N, k):
        key = self._make_key(N, k)
        return self.epoch_level_metrics[metric_name][key]

    def get_metrics(self, N, k):
        """Returns a dictionary of metrics of the N-way, k-shot experiment."""
        result = dict()
        for metric_name in self.epoch_level_metrics:
            result[metric_name] = self.get_metric(metric_name, N, k)

        return result

    def get_best_embeddings(self, N, k):
        """Get the embeddings of test and target dataset at the best model."""
        key = self._make_key(N, k)
        target_embeddings = self.best_model_metrics["target_embeddings"][key]
        target_labels = self.best_model_metrics["target_labels"][key]
        test_embeddings = self.best_model_metrics["test_embeddings"][key]
        test_labels = self.best_model_metrics["test_labels"][key]

        target_embeddings = np.array(target_embeddings)
        target_labels = np.array(target_labels)
        test_embeddings = np.array(test_embeddings)
        test_labels = np.array(test_labels)

        return (target_embeddings, target_labels), (test_embeddings, test_labels)

    def update_inference_time(self, inference_time):
        current_inference_time = self.training_information["inference_time"]
        self.training_information["inference_time"] = max(
            current_inference_time, inference_time)

    def record_metric(self, metric_name, N, k, value):
        """Record the metrics for epochs level metrics.

        Args:
            metric_name (String): The name of the metrics to record.
            N (int): The number of unseen class.
            k (int): The number of targets per unseen classes.
            value (Object): The value that we want to record.
        """
        key = self._make_key(N, k)
        self.epoch_level_metrics[metric_name][key].append(value)

    def update_best_metric(self, metrics_name, N, k, value):
        """Update the metrics for best model metrics.

        Args:
            metric_name (String): The name of the metrics to update.
            N (int): The number of unseen class.
            k (int): The number of targets per unseen classes.
            value (Object): The value that we want to update.
        """
        key = self._make_key(N, k)
        self.best_model_metrics[metrics_name][key] = value

    def should_update_best_metrics(self, N, k):
        # Accuracy is the current proxy on what is the best model as current
        # we are not able to compute the similarity loss for test dataset.
        comparsion_metric = "accuracy"
        key = self._make_key(N, k)
        current_value = self.epoch_level_metrics[comparsion_metric][key][-1]
        best_value = self.best_model_metrics[comparsion_metric][key]

        # the current proxy is accuracy so higher the better
        return current_value > best_value

    def to_json(self):
        return json.dumps(self.__dict__)

    @classmethod
    def from_json(cls, json_obj):
        result = cls()
        result.model_information = json_obj.get('model_information', {})
        result.data_information = json_obj.get('data_information', {})
        result.training_information = json_obj.get('training_information', {})

        if "epoch_level_metrics" in json_obj:
            result.epoch_level_metrics = json_obj["epoch_level_metrics"]
        if "best_model_metrics" in json_obj:
            result.best_model_metrics = json_obj["best_model_metrics"]

        return result

    @classmethod
    def from_string(cls, json_str):
        json_obj = json.loads(json_str)
        return cls.from_json(json_obj)

    @classmethod
    def read(cls, filename):
        with open(filename, "rt") as f:
            json_obj = json.load(f)
            return cls.from_json(json_obj)

    def download_to_disk(
            self,
            path="./benchmark_metrics",
            file_name="results.json"):
        """Download the Result to disk in JSON format

        Args:
            path (str, optional): The path that we want to store the Result.
                Defaults to "./benchmark_metrics".
            file_name (str, optional): The file name we want to store it as.
                Defaults to "results.json".
        """

        # Create a directory at log_dir if it does not exist
        os.makedirs(path, exist_ok=True)
        file_path = os.path.join(path, file_name)

        with open(file_path, 'w+') as f:
            json.dump(self.__dict__, f)

    def upload_to_bigquery(self, dataset_id, table_id):
        pass

    def _make_key(self, N, k):
        return "{}_ways_{}_shots".format(N, k)
