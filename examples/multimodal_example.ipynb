{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "252467c5",
      "metadata": {
        "id": "252467c5"
      },
      "source": [
        "Consider that you have a online art gallery with thousands of paintings. How would you search through it? You can search for artists or painting titles but what if you want to search for a painting by description?\n",
        "Descriptions may not be available for all paintings. You can get descriptions written manually for each painting but it would be an huge amount of work and would slow down the digitization process. What if we could search through images of paintings without manually labelling them?\n",
        "For example, if you search for a painting of sea waves in a Japenese style, you would expect to see something like this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b54b1cb",
      "metadata": {
        "id": "5b54b1cb"
      },
      "source": [
        "<img src=\"https://www.artic.edu/iiif/2/b3974542-b9b4-7568-fc4b-966738f61d78/full/1686,/0/default.jpg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ff5f5d",
      "metadata": {
        "id": "e5ff5f5d"
      },
      "source": [
        "We are going to train a image encoder model which can be used to index the images, and we can train a text encoder model which can encode our search query. We can then find our image by finding the nearest image vector to our text vector."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad1a349",
      "metadata": {
        "id": "2ad1a349"
      },
      "source": [
        "For this task, we are going to finetune CLIP model which was trained on a large training dataset and hence has good zero shot preformance. We will take pretrained model as a baseline and see if we can improve its performance on our task by finetuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6qvOpA_LajsZ",
      "metadata": {
        "id": "6qvOpA_LajsZ"
      },
      "source": [
        "We are going to use dataset from [Art Institute of Chicago](https://www.artic.edu/) to finetune our model. It has images of artworks and information such as title, artist, alt_text and description. We are going to use descriptions as they are more suitable for our problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a05cdbc",
      "metadata": {
        "id": "2a05cdbc"
      },
      "source": [
        "Let's import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "854c65ce",
      "metadata": {
        "id": "854c65ce"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8941bc45",
      "metadata": {
        "id": "8941bc45"
      },
      "outputs": [],
      "source": [
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b32d6761",
      "metadata": {
        "id": "b32d6761"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "78a778a0",
      "metadata": {
        "id": "78a778a0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gIlFIhcappKp",
      "metadata": {
        "id": "gIlFIhcappKp"
      },
      "outputs": [],
      "source": [
        "# Installing dependencies\n",
        "!pip install transformers\n",
        "\n",
        "!pip install git+https://github.com/abhisharsinha/similarity@development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a53ebbbd",
      "metadata": {
        "id": "a53ebbbd"
      },
      "outputs": [],
      "source": [
        "from tensorflow_similarity.losses import MultiNegativesRankLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e01ec309",
      "metadata": {
        "id": "e01ec309"
      },
      "outputs": [],
      "source": [
        "from transformers import TFCLIPTextModel, TFCLIPVisionModel, CLIPTokenizer, TFCLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "53b4e476",
      "metadata": {
        "id": "53b4e476"
      },
      "outputs": [],
      "source": [
        "from tensorflow.data.experimental import AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9490b93a",
      "metadata": {
        "id": "9490b93a"
      },
      "outputs": [],
      "source": [
        "n_cpu = multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "818ce1c6",
      "metadata": {
        "id": "818ce1c6"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "COLOR_CHANNELS = 3\n",
        "N_TOKENS = 77"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JJRebGa4pdMu",
      "metadata": {
        "id": "JJRebGa4pdMu"
      },
      "source": [
        "Doownloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "jWsG-qx3Aj5Q",
      "metadata": {
        "id": "jWsG-qx3Aj5Q"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/datasets/abhishars/artic-dataset/resolve/main/artworks.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1yFuCmckpgIc",
      "metadata": {
        "id": "1yFuCmckpgIc"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/mys-released-models/gsoc/artic-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "kOvtgplMpioU",
      "metadata": {
        "id": "kOvtgplMpioU"
      },
      "outputs": [],
      "source": [
        "!unzip artic-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96527c38",
      "metadata": {},
      "source": [
        "We want to get embeddings of images and texts such that the distance between image and it's description is lesser than the image and description of any other image.\n",
        "First, we need a way to get text and image embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91df9831",
      "metadata": {
        "id": "91df9831"
      },
      "source": [
        "Getting weights of projection layers after vision and text encoder models of CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5d150f97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d150f97",
        "outputId": "a6c85b0b-85ae-4cd6-da0f-255095256b5e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFCLIPModel.\n",
            "\n",
            "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "vision_weights = tf.Variable(model.weights[-2])\n",
        "text_weights = tf.Variable(model.weights[-1])\n",
        "del model\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8afe43",
      "metadata": {},
      "source": [
        "We'll use CLIPTextModel and CLIPVisionMOdel to embed text and images respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab7a609",
      "metadata": {
        "id": "3ab7a609"
      },
      "source": [
        "Loading pretrained CLIP text encoder model and the corresponding tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579405e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "579405e4",
        "outputId": "8be7c5bf-a60b-4bae-e7b2-4a3883229afe"
      },
      "outputs": [],
      "source": [
        "CLIP_text_model = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\", )\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448655c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "448655c3",
        "outputId": "928eb612-e96d-42b3-93b2-c72e20988c6a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "CLIP_vision_model = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\",  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97777fed",
      "metadata": {
        "id": "97777fed"
      },
      "source": [
        "If you have GPU restrictions you can set the base models as non-trainable and only train the projector layer for a faster optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3c7f1475",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c7f1475",
        "outputId": "6063b919-87d6-4566-d2b5-af904d0463bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tfclip_vision_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " clip (TFCLIPVisionMainLayer  multiple                 87456000  \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87,456,000\n",
            "Trainable params: 87,456,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# CLIP_vision_model.trainable = False\n",
        "CLIP_vision_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dcca85eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcca85eb",
        "outputId": "321052ef-46e2-4cc6-89e2-c0a630da4a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tfclip_text_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " clip (TFCLIPTextMainLayer)  multiple                  63165952  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63,165,952\n",
            "Trainable params: 63,165,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# CLIP_text_model.trainable = False\n",
        "CLIP_text_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b38d13",
      "metadata": {},
      "source": [
        "Since the embedding size of CLIPVisionModel and CLIPTextModel are different, we need projection layers at the top of both these models to make embedding of same dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f9f3f5",
      "metadata": {
        "id": "82f9f3f5"
      },
      "outputs": [],
      "source": [
        "def get_image_model(n_dims=512):\n",
        "    x = tf.keras.layers.Input((COLOR_CHANNELS, IMG_SIZE, IMG_SIZE),\n",
        "                              name=\"image\")\n",
        "    vision_outputs = CLIP_vision_model(x)\n",
        "    vision_last_hidden_state = vision_outputs.last_hidden_state\n",
        "    vision_pooled_output = vision_outputs.pooler_output  # pooled CLS states\n",
        "    vision_init = tf.constant_initializer(vision_weights.numpy())\n",
        "    img_embed = tf.keras.layers.Dense(n_dims,\n",
        "                                      name=\"image_embedding\",\n",
        "                                      kernel_initializer=vision_init\n",
        "                                      )(vision_pooled_output)\n",
        "    img_model = tf.keras.models.Model(inputs=x,\n",
        "                                      outputs=img_embed,\n",
        "                                      name=\"image_model\")\n",
        "\n",
        "    return img_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c0e31cb",
      "metadata": {
        "id": "0c0e31cb"
      },
      "outputs": [],
      "source": [
        "def get_text_model(n_dims=512):\n",
        "    x1 = tf.keras.layers.Input((N_TOKENS),\n",
        "                               dtype=tf.int32,\n",
        "                               name=\"input_ids\")\n",
        "    x2 = tf.keras.layers.Input((N_TOKENS),\n",
        "                               dtype=tf.int32,\n",
        "                               name=\"attention_mask\")\n",
        "    text_outputs = CLIP_text_model(input_ids=x1, attention_mask=x2)\n",
        "    text_last_hidden_state = text_outputs.last_hidden_state\n",
        "    text_pooled_output = text_outputs.pooler_output\n",
        "    text_init = tf.constant_initializer(text_weights.numpy())\n",
        "    text_embed = tf.keras.layers.Dense(n_dims,\n",
        "                                       name=\"text_embedding\",\n",
        "                                       kernel_initializer=text_init\n",
        "                                       )(text_pooled_output)\n",
        "    text_model = tf.keras.models.Model(inputs=[x1, x2],\n",
        "                                       outputs=text_embed,\n",
        "                                       name=\"text_model\")\n",
        "\n",
        "    return text_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dae9edd",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "img_model = get_image_model()\n",
        "text_model = get_text_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0af467a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0af467a",
        "outputId": "91cd68a2-49f8-4775-999d-078a1264c54a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"image_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " image (InputLayer)          [(None, 3, 224, 224)]     0         \n",
            "                                                                 \n",
            " tfclip_vision_model (TFCLIP  TFBaseModelOutputWithPoo  87456000 \n",
            " VisionModel)                ling(last_hidden_state=(            \n",
            "                             None, 50, 768),                     \n",
            "                              pooler_output=(None, 76            \n",
            "                             8),                                 \n",
            "                              hidden_states=None, att            \n",
            "                             entions=None)                       \n",
            "                                                                 \n",
            " image_embedding (Dense)     (None, 512)               393728    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87,849,728\n",
            "Trainable params: 87,849,728\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "img_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c47add2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c47add2",
        "outputId": "e8bb08e4-0b65-403d-d1b7-df3dbeed79e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 77)]         0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 77)]         0           []                               \n",
            "                                                                                                  \n",
            " tfclip_text_model (TFCLIPTextM  TFBaseModelOutputWi  63165952   ['input_ids[0][0]',              \n",
            " odel)                          thPooling(last_hidd               'attention_mask[0][0]']         \n",
            "                                en_state=(None, 77,                                               \n",
            "                                 512),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 512),                                                          \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " text_embedding (Dense)         (None, 512)          262656      ['tfclip_text_model[0][1]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 63,428,608\n",
            "Trainable params: 63,428,608\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "text_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GGbVpXhMxXzw",
      "metadata": {
        "id": "GGbVpXhMxXzw"
      },
      "source": [
        "Now let's load our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d3bbb2ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "d3bbb2ec",
        "outputId": "d507b727-4384-4c00-bf87-7787cff6152a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b97169fd-a050-4315-822e-ed9cce3e1297\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>art_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>alt_text</th>\n",
              "      <th>description</th>\n",
              "      <th>full_description</th>\n",
              "      <th>artist</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>795</td>\n",
              "      <td>2fa88251-065a-8c94-d918-63bfd5c866c0</td>\n",
              "      <td>Rectangular, mostly brown textile made up of m...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kesa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1936</td>\n",
              "      <td>1733f6b2-2cac-e8fe-4ecd-c0587da78b30</td>\n",
              "      <td>A work made of silk, plain compound cloth.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fragment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4425</td>\n",
              "      <td>0da94f5f-4be8-a3d0-00c0-53d48ad4a2cf</td>\n",
              "      <td>A work made of color woodblock print; chuban.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Utagawa Hiroshige</td>\n",
              "      <td>Kyoto: The Imperial Palace (Kyo, Dairi), from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5431</td>\n",
              "      <td>121a5968-7d19-5879-8974-b6a6bdaa5d1f</td>\n",
              "      <td>A work made of black crayon, with touches of c...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>László Moholy-Nagy</td>\n",
              "      <td>Seated Woman II</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8101</td>\n",
              "      <td>c24feb4e-d7f5-791e-58ee-5db1a40d0a0d</td>\n",
              "      <td>A work made of cotton, plain weave; drawnwork ...</td>\n",
              "      <td>This apron mimics lace in its play of transluc...</td>\n",
              "      <td>This apron mimics lace in its play of transluc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10393</td>\n",
              "      <td>357c901c-c54f-079c-0c29-d3be0a021ed4</td>\n",
              "      <td>A work made of porcelain painted in underglaze...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Deep Dish with Peony, Pine Branches, Plum Blos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10924</td>\n",
              "      <td>179ae0da-8834-7c92-3b25-377a08dac9e4</td>\n",
              "      <td>A work made of blown glass.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Artist unknown</td>\n",
              "      <td>Cream Pitcher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11460</td>\n",
              "      <td>b84f047e-f871-48c5-6bbf-618731650105</td>\n",
              "      <td>A work made of engraving in black on ivory lai...</td>\n",
              "      <td>The ', &lt;a href='https://www.artic.edu/artists/...</td>\n",
              "      <td>The ', &lt;a href='https://www.artic.edu/artists/...</td>\n",
              "      <td>Master of the E-Series Tarocchi</td>\n",
              "      <td>Philosophy, plate 28 from Arts and Sciences</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11785</td>\n",
              "      <td>04ed0f69-131f-bf49-b791-22e9cb692adb</td>\n",
              "      <td>A work made of etching and drypoint in black o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Donald Shaw MacLaughlan</td>\n",
              "      <td>A Lion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12707</td>\n",
              "      <td>22306679-f60a-1c18-4a7f-08720d17aab2</td>\n",
              "      <td>A work made of cotton, mixed lace: bobbin lace...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Stole</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b97169fd-a050-4315-822e-ed9cce3e1297')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b97169fd-a050-4315-822e-ed9cce3e1297 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b97169fd-a050-4315-822e-ed9cce3e1297');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   art_id                              image_id  \\\n",
              "0     795  2fa88251-065a-8c94-d918-63bfd5c866c0   \n",
              "1    1936  1733f6b2-2cac-e8fe-4ecd-c0587da78b30   \n",
              "2    4425  0da94f5f-4be8-a3d0-00c0-53d48ad4a2cf   \n",
              "3    5431  121a5968-7d19-5879-8974-b6a6bdaa5d1f   \n",
              "4    8101  c24feb4e-d7f5-791e-58ee-5db1a40d0a0d   \n",
              "5   10393  357c901c-c54f-079c-0c29-d3be0a021ed4   \n",
              "6   10924  179ae0da-8834-7c92-3b25-377a08dac9e4   \n",
              "7   11460  b84f047e-f871-48c5-6bbf-618731650105   \n",
              "8   11785  04ed0f69-131f-bf49-b791-22e9cb692adb   \n",
              "9   12707  22306679-f60a-1c18-4a7f-08720d17aab2   \n",
              "\n",
              "                                            alt_text  \\\n",
              "0  Rectangular, mostly brown textile made up of m...   \n",
              "1         A work made of silk, plain compound cloth.   \n",
              "2      A work made of color woodblock print; chuban.   \n",
              "3  A work made of black crayon, with touches of c...   \n",
              "4  A work made of cotton, plain weave; drawnwork ...   \n",
              "5  A work made of porcelain painted in underglaze...   \n",
              "6                        A work made of blown glass.   \n",
              "7  A work made of engraving in black on ivory lai...   \n",
              "8  A work made of etching and drypoint in black o...   \n",
              "9  A work made of cotton, mixed lace: bobbin lace...   \n",
              "\n",
              "                                         description  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4  This apron mimics lace in its play of transluc...   \n",
              "5                                                NaN   \n",
              "6                                                NaN   \n",
              "7  The ', <a href='https://www.artic.edu/artists/...   \n",
              "8                                                NaN   \n",
              "9                                                NaN   \n",
              "\n",
              "                                    full_description  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4  This apron mimics lace in its play of transluc...   \n",
              "5                                                NaN   \n",
              "6                                                NaN   \n",
              "7  The ', <a href='https://www.artic.edu/artists/...   \n",
              "8                                                NaN   \n",
              "9                                                NaN   \n",
              "\n",
              "                            artist  \\\n",
              "0                              NaN   \n",
              "1                              NaN   \n",
              "2                Utagawa Hiroshige   \n",
              "3               László Moholy-Nagy   \n",
              "4                              NaN   \n",
              "5                              NaN   \n",
              "6                   Artist unknown   \n",
              "7  Master of the E-Series Tarocchi   \n",
              "8          Donald Shaw MacLaughlan   \n",
              "9                              NaN   \n",
              "\n",
              "                                               title  \n",
              "0                                               Kesa  \n",
              "1                                           Fragment  \n",
              "2  Kyoto: The Imperial Palace (Kyo, Dairi), from ...  \n",
              "3                                    Seated Woman II  \n",
              "4                                              Apron  \n",
              "5  Deep Dish with Peony, Pine Branches, Plum Blos...  \n",
              "6                                      Cream Pitcher  \n",
              "7        Philosophy, plate 28 from Arts and Sciences  \n",
              "8                                             A Lion  \n",
              "9                                              Stole  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "art_df = pd.read_csv(\"artworks.csv\")\n",
        "art_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9782fb43",
      "metadata": {
        "id": "9782fb43"
      },
      "outputs": [],
      "source": [
        "art_df.drop_duplicates(subset=[\"description\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4f49034a",
      "metadata": {
        "id": "4f49034a"
      },
      "outputs": [],
      "source": [
        "art_df.dropna(subset=[\"description\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "742f426c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "742f426c",
        "outputId": "c93c011d-b92e-4bde-f0aa-617d4646c044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "art_id              7005\n",
              "image_id            7005\n",
              "alt_text            7005\n",
              "description         7005\n",
              "full_description    7005\n",
              "artist              6467\n",
              "title               7005\n",
              "dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "art_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3c46d48c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c46d48c",
        "outputId": "72fcf867-4212-441f-b747-9816ac984220"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37649"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_images = set(list(os.listdir(\"artic-dataset/\")))\n",
        "len(all_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2df9ab04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2df9ab04",
        "outputId": "7b5395a7-1c57-4eae-ac40-3e7651b1ac0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7005it [00:00, 8217.53it/s]\n"
          ]
        }
      ],
      "source": [
        "imgs_list = []\n",
        "text_list = []\n",
        "for i in tqdm(art_df.iterrows()):\n",
        "    if i[1][\"image_id\"]+\".jpg\" in all_images:\n",
        "        imgs_list.append(\"artic-dataset/{}.jpg\".format(i[1][\"image_id\"]))\n",
        "        # tokenizer cannot take tf str so pretokenizing\n",
        "        text_list.append(i[1][\"description\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f4bf604e",
      "metadata": {
        "id": "f4bf604e"
      },
      "outputs": [],
      "source": [
        "train_images, val_images, train_texts, val_texts = train_test_split(imgs_list, text_list, test_size=0.2, random_state=17)\n",
        "\n",
        "train_tokens = tokenizer(train_texts,\n",
        "                    padding=\"max_length\",\n",
        "                    return_tensors=\"tf\",\n",
        "                    truncation=True,\n",
        "                    )\n",
        "\n",
        "val_tokens = tokenizer(val_texts,\n",
        "                    padding=\"max_length\",\n",
        "                    return_tensors=\"tf\",\n",
        "                    truncation=True,\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "80cb9527",
      "metadata": {
        "id": "80cb9527"
      },
      "outputs": [],
      "source": [
        "def get_img_emb(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method=\"nearest\")\n",
        "    image = tf.transpose(image, [2, 0, 1]) # Channels first\n",
        "    return image\n",
        "\n",
        "\n",
        "def data_mapper(img, input_ids, attention_mask):\n",
        "    return get_img_emb(img), tf.squeeze(input_ids), tf.squeeze(attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0219d4a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0219d4a1",
        "outputId": "deec3a16-5579-461f-c7db-cfbd485db326",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 3, 224, 224)\n",
            "(64, 77)\n",
            "(64, 77)\n",
            "(64, 3, 224, 224)\n",
            "(64, 77)\n",
            "(64, 77)\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "                            (train_images,\n",
        "                             train_tokens[\"input_ids\"],\n",
        "                             train_tokens[\"attention_mask\"])\n",
        "                            )\n",
        "train_ds = train_ds.map(data_mapper, num_parallel_calls=AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(2000).batch(BATCH_SIZE).cache().prefetch(AUTOTUNE)\n",
        "for i in train_ds.take(1):\n",
        "    for j in i:\n",
        "        print(j.shape)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_images,\n",
        "                             val_tokens[\"input_ids\"],\n",
        "                             val_tokens[\"attention_mask\"])\n",
        "                            )\n",
        "val_ds = val_ds.map(data_mapper, num_parallel_calls=n_cpu).batch(BATCH_SIZE).cache()\n",
        "for i in val_ds.take(1):\n",
        "    for j in i:\n",
        "        print(j.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "81eb900b",
      "metadata": {
        "id": "81eb900b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c1860e54",
      "metadata": {
        "id": "c1860e54"
      },
      "source": [
        "We have a dataset which have pairs of image and text. We only have positive examples here and in such a case multiple negatives ranking loss is a suitable choice. This considers every other pair other than ($x_i$, $y_i$) as negative pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cd4e3733",
      "metadata": {
        "id": "cd4e3733"
      },
      "outputs": [],
      "source": [
        "loss_fn = MultiNegativesRankLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877b6183",
      "metadata": {
        "id": "877b6183"
      },
      "source": [
        "We'll use R@k metric for evaluation. This is a common metric used in evaluation of ranking. It is the average of recall by taking top k predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "a5da6f8f",
      "metadata": {
        "id": "a5da6f8f"
      },
      "outputs": [],
      "source": [
        "def recall_at_k(sim_matrix, k=1):\n",
        "    \"\"\" \n",
        "        It is the mean of ratio of correctly retrieved documents\n",
        "        to the number of relevant documents.\n",
        "        This implementation is specific to\n",
        "        data having unique label for each key\n",
        "    \"\"\"\n",
        "    \n",
        "    sorted_mat = np.argsort(sim_matrix, axis=1)[:,-k:]\n",
        "    \n",
        "    # Each key has unique label\n",
        "    true_labels = np.arange(sorted_mat.shape[0]).reshape(-1,1)\n",
        "    true_labels = np.repeat(true_labels, k, axis=1)\n",
        "    sorted_mat = sorted_mat - true_labels\n",
        "    # the position in row corresponding to true positive\n",
        "    # will be zero\n",
        "    tps = np.any(sorted_mat == 0, axis=1)\n",
        "    return tps.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wI5uQIC8xG4z",
      "metadata": {
        "id": "wI5uQIC8xG4z"
      },
      "source": [
        "Let's measure the baseline performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "998018f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "998018f7",
        "outputId": "aa6b4c15-8105-4454-aef5-b096b843841a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:09<00:00,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6188373348929663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "val_loss = 0\n",
        "base_image_embeddings = []\n",
        "base_text_embeddings = []\n",
        "for image_batch, input_ids_batch, attention_mask_batch in tqdm(val_ds):\n",
        "    image_embedding = img_model(image_batch, training=False)\n",
        "    text_embedding = text_model([input_ids_batch, attention_mask_batch], training=False)\n",
        "\n",
        "    image_embedding = tf.math.l2_normalize(image_embedding, axis=1)\n",
        "    text_embedding = tf.math.l2_normalize(text_embedding, axis=1)\n",
        "\n",
        "    base_image_embeddings.append(image_embedding.numpy())\n",
        "    base_text_embeddings.append(text_embedding.numpy())\n",
        "    \n",
        "    # Compute the loss value for this minibatch.\n",
        "    loss_value = loss_fn(text_embedding, image_embedding)\n",
        "    val_loss += float(loss_value)\n",
        "print(val_loss/len(val_ds))\n",
        "base_image_embeddings = np.concatenate(base_image_embeddings)\n",
        "base_text_embeddings = np.concatenate(base_text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "gNrGG0qYc4Qg",
      "metadata": {
        "id": "gNrGG0qYc4Qg"
      },
      "outputs": [],
      "source": [
        "base_sim_mat = np.matmul(base_text_embeddings, base_image_embeddings.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "82b1b414",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82b1b414",
        "outputId": "fce48db4-ce50-4a7c-b603-738ad7a8b1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R@1: 0.1841541755888651\n",
            "R@2: 0.2676659528907923\n",
            "R@3: 0.31977159172019987\n",
            "R@4: 0.3576017130620985\n",
            "R@5: 0.3811563169164882\n"
          ]
        }
      ],
      "source": [
        "for k in range(1, 6):\n",
        "    print(\"R@{}: {}\".format(k, recall_at_k(base_sim_mat, k)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GFPy9nhaehJT",
      "metadata": {
        "id": "GFPy9nhaehJT"
      },
      "source": [
        "Now let's train the models.\n",
        "Since we have two models that need to be trained together, we'll use custom training loop similar to [this guide](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#end-to-end_example_a_gan_training_loop_from_scratch)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6792a9f5",
      "metadata": {
        "id": "6792a9f5"
      },
      "source": [
        "Let's define the train_step where model's weights are updated based on a batch and then start the training loop in which each epochs calls the train_step for every batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "851709f1",
      "metadata": {
        "id": "851709f1"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(image_batch, text_batch):\n",
        "    with tf.GradientTape() as img_tape, tf.GradientTape() as text_tape:\n",
        "            \n",
        "        image_embedding = img_model(image_batch, training=True)\n",
        "        text_embedding = text_model(text_batch, training=True)\n",
        "\n",
        "        image_embedding = tf.math.l2_normalize(image_embedding, axis=1)\n",
        "        text_embedding = tf.math.l2_normalize(text_embedding, axis=1)\n",
        "\n",
        "        # Compute the loss value for this minibatch.\n",
        "        loss_value = loss_fn(text_embedding, image_embedding)\n",
        "\n",
        "    img_grads = img_tape.gradient(loss_value, img_model.trainable_weights)\n",
        "    text_grads = text_tape.gradient(loss_value, text_model.trainable_weights)\n",
        "\n",
        "    # Run one step of gradient descent by updating\n",
        "    # the value of the variables to minimize the loss.\n",
        "    img_optimizer.apply_gradients(zip(img_grads, img_model.trainable_weights))\n",
        "    text_optimizer.apply_gradients(zip(text_grads, text_model.trainable_weights))\n",
        "\n",
        "    return loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8fa53101",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fa53101",
        "outputId": "8be0e0cd-9212-4f1c-ae63-c886f0b00c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training loss (for one batch) at step 1: 2.6298\n",
            "Seen so far: 64 samples\n",
            "Epoch loss: 1.248113751411438\n",
            "\n",
            "Epoch 2\n",
            "Training loss (for one batch) at step 1: 0.5429\n",
            "Seen so far: 64 samples\n",
            "Epoch loss: 0.3632849793965844\n",
            "\n",
            "Epoch 3\n",
            "Training loss (for one batch) at step 1: 0.1357\n",
            "Seen so far: 64 samples\n",
            "Epoch loss: 0.1251609335357154\n",
            "\n",
            "Epoch 4\n",
            "Training loss (for one batch) at step 1: 0.0729\n",
            "Seen so far: 64 samples\n",
            "Epoch loss: 0.06818865270311521\n",
            "\n",
            "Epoch 5\n",
            "Training loss (for one batch) at step 1: 0.0656\n",
            "Seen so far: 64 samples\n",
            "Epoch loss: 0.03815177821723575\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "img_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "text_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "train_step_losses = []\n",
        "train_epoch_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nEpoch %d\" % (epoch+1,))\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (image_batch, input_ids_batch, attention_mask_batch) in enumerate(train_ds):\n",
        "        loss_value = train_step(image_batch, [input_ids_batch, attention_mask_batch])\n",
        "        epoch_loss += float(loss_value)\n",
        "        train_step_losses.append(float(loss_value)/image_batch.shape[0])\n",
        "        # Log every batch\n",
        "        if step % 100 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step+1, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * BATCH_SIZE))\n",
        "    print(\"Epoch loss:\", epoch_loss/len(train_ds))\n",
        "    train_epoch_losses.append(epoch_loss/len(train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "ec7ceaa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ec7ceaa3",
        "outputId": "e5e056ce-e8c0-42b2-edb0-91f7d9eb103f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_epoch_losses)\n",
        "plt.title(\"Training\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0f09ad3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "0f09ad3a",
        "outputId": "2b811f7d-c964-4a34-88fc-16fb11eba518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_step_losses)\n",
        "plt.title(\"Training\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YBl3IQe5t8Dt",
      "metadata": {
        "id": "YBl3IQe5t8Dt"
      },
      "source": [
        "Now let's check the performance on validation data after finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a32c74dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a32c74dc",
        "outputId": "e932916f-d0ca-4b21-8823-1ec7533afd0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:10<00:00,  2.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.992127467285503\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "val_loss = 0\n",
        "image_embeddings = []\n",
        "text_embeddings = []\n",
        "for image_batch, input_ids_batch, attention_mask_batch in tqdm(val_ds):\n",
        "    image_embedding = img_model(image_batch, training=False)\n",
        "    text_embedding = text_model([input_ids_batch, attention_mask_batch], training=False)\n",
        "\n",
        "    image_embedding = tf.math.l2_normalize(image_embedding, axis=1)\n",
        "    text_embedding = tf.math.l2_normalize(text_embedding, axis=1)\n",
        "\n",
        "    image_embeddings.append(image_embedding.numpy())\n",
        "    text_embeddings.append(text_embedding.numpy())\n",
        "    \n",
        "    # Compute the loss value for this minibatch.\n",
        "    loss_value = loss_fn(text_embedding, image_embedding)\n",
        "    val_loss += float(loss_value)\n",
        "print(val_loss/len(val_ds))\n",
        "image_embeddings = np.concatenate(image_embeddings)\n",
        "text_embeddings = np.concatenate(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "bsD2IkvdZA2N",
      "metadata": {
        "id": "bsD2IkvdZA2N"
      },
      "outputs": [],
      "source": [
        "finetuned_sim = np.matmul(text_embeddings, image_embeddings.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "7c60953d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c60953d",
        "outputId": "373e2f79-2d9c-4c63-f999-dcd08b93b89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R@1 : 0.2955032119914347\n",
            "R@2 : 0.39543183440399715\n",
            "R@3 : 0.4732334047109208\n",
            "R@4 : 0.5339043540328337\n",
            "R@5 : 0.5724482512491078\n"
          ]
        }
      ],
      "source": [
        "for k in range(1,6):\n",
        "    print(\"R@{} : {}\".format(k, recall_at_k(finetuned_sim, k)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8xwd13weIVP",
      "metadata": {
        "id": "a8xwd13weIVP"
      },
      "source": [
        "We can see that thre is a significant imrovement in recall metric after only 5 epochs of training.\n",
        "Now, we can use these modes for painting retrival based on text query."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa370eb1",
      "metadata": {
        "id": "aa370eb1"
      },
      "source": [
        "The models can be saved in the same way as any other keras model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6717ca",
      "metadata": {
        "id": "7f6717ca"
      },
      "outputs": [],
      "source": [
        "img_model.save(\"image_model.h5\")\n",
        "text_model.save(\"text_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d2e778",
      "metadata": {
        "id": "a2d2e778"
      },
      "source": [
        "However when loading these saved models, the CLIP vision and text models must be provided in custom objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0fb4c52",
      "metadata": {
        "id": "e0fb4c52"
      },
      "outputs": [],
      "source": [
        "img_model = tf.keras.models.load_model(\"image_model.h5\", custom_objects={\"TFCLIPVisionModel\":TFCLIPVisionModel})\n",
        "text_model = tf.keras.models.load_model(\"text_model.h5\", custom_objects={\"TFCLIPTextModel\":TFCLIPTextModel})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
