{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "008b0470",
   "metadata": {},
   "source": [
    "Copyright 2023 abhisharsinha and The TensorFlow Similarity Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b6f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e22169f",
   "metadata": {},
   "source": [
    "# TensorFlow Similarity MultiModal Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "252467c5",
   "metadata": {
    "id": "252467c5"
   },
   "source": [
    "Imagine that you're running an online art gallery with thousands of paintings. You've allowed your customers to search for paintings by artists or painting titles, but what if they want to find paintings based on their descriptions? Unfortunately, not all paintings come with descriptions, and manually writing them would require a massive amount of effort and slow down the digitization process. So, what if we could search for paintings by analyzing their images, without having to label them manually? For instance, if a customer searches for a painting of sea waves in a Japanese style, they would expect to see something like this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b54b1cb",
   "metadata": {
    "id": "5b54b1cb"
   },
   "source": [
    "<img src=\"https://www.artic.edu/iiif/2/b3974542-b9b4-7568-fc4b-966738f61d78/full/835,/0/default.jpg\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ad1a349",
   "metadata": {
    "id": "2ad1a349"
   },
   "source": [
    "### Notebook goal\n",
    "\n",
    "We will be training an image encoder model that indexes images, as well as a text encoder model that encodes our search queries. With these models, we can locate an image by finding the nearest image vector to our text vector. To accomplish this, we will fine-tune the CLIP model, which was trained on a large dataset and has demonstrated impressive zero-shot performance. Our approach will be to use a pre-trained model as a baseline, and attempt to improve its performance on our task through fine-tuning. To accomplish this, we will use a dataset from the [Art Institute of Chicago](https://www.artic.edu/), which contains images of artworks along with metadata such as the title, artist, alt_text, and description. We will use the descriptions, as they are the most suitable for our specific problem.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27ce8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "# from tabulate import tabulate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# INFO messages are not printed.\n",
    "# This must be run before loading other modules.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7188af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083b3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install TF similarity if needed\n",
    "try:\n",
    "    import tensorflow_similarity as tfsim  # main package\n",
    "except ModuleNotFoundError:\n",
    "    !pip install tensorflow_similarity\n",
    "    import tensorflow_similarity as tfsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044843d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Transformer deps from hugging face\n",
    "try:\n",
    "    from transformers import TFCLIPTextModel, TFCLIPVisionModel, CLIPTokenizer, TFCLIPModel\n",
    "except ModuleNotFoundError:\n",
    "    !pip install transformers\n",
    "    from transformers import TFCLIPTextModel, TFCLIPVisionModel, CLIPTokenizer, TFCLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2355313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfsim.utils.tf_cap_memory()\n",
    "# Clear out any old model state.\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6924a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.12.0\n",
      "TensorFlow Similarity 0.18.0.dev8\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"TensorFlow Similarity\", tfsim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818ce1c6",
   "metadata": {
    "id": "818ce1c6"
   },
   "outputs": [],
   "source": [
    "N_CPU = os.cpu_count()\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "COLOR_CHANNELS = 3\n",
    "N_TOKENS = 77\n",
    "DATA_DIR = \"multi_modal_datasets\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "JJRebGa4pdMu",
   "metadata": {
    "id": "JJRebGa4pdMu"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f8a8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "jWsG-qx3Aj5Q",
   "metadata": {
    "id": "jWsG-qx3Aj5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘multi_modal_datasets/artworks.csv’ already there; not retrieving.\n",
      "\n",
      "File ‘multi_modal_datasets/artic-dataset.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  multi_modal_datasets/artic-dataset.zip\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://huggingface.co/datasets/abhishars/artic-dataset/resolve/main/artworks.csv -P {DATA_DIR}\n",
    "!wget -nc https://storage.googleapis.com/mys-released-models/gsoc/artic-dataset.zip -P {DATA_DIR}\n",
    "!unzip -n {DATA_DIR}/artic-dataset.zip -d {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c46d48c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c46d48c",
    "outputId": "72fcf867-4212-441f-b747-9816ac984220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37649 unique image ids\n"
     ]
    }
   ],
   "source": [
    "# Get the set of unique image ids in the unzipped dir\n",
    "ARTIC_IMAGE_DIR = os.path.join(DATA_DIR, \"artic-dataset\")\n",
    "image_ids = [os.path.splitext(fn)[0] for fn in os.listdir(ARTIC_IMAGE_DIR)]\n",
    "image_ids = set(image_ids)\n",
    "\n",
    "print(f\"There are {len(image_ids)} unique image ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3bbb2ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "d3bbb2ec",
    "outputId": "d507b727-4384-4c00-bf87-7787cff6152a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>description</th>\n",
       "      <th>full_description</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8101</td>\n",
       "      <td>c24feb4e-d7f5-791e-58ee-5db1a40d0a0d</td>\n",
       "      <td>A work made of cotton, plain weave; drawnwork ...</td>\n",
       "      <td>This apron mimics lace in its play of transluc...</td>\n",
       "      <td>This apron mimics lace in its play of transluc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11460</td>\n",
       "      <td>b84f047e-f871-48c5-6bbf-618731650105</td>\n",
       "      <td>A work made of engraving in black on ivory lai...</td>\n",
       "      <td>The ', &lt;a href='https://www.artic.edu/artists/...</td>\n",
       "      <td>The ', &lt;a href='https://www.artic.edu/artists/...</td>\n",
       "      <td>Master of the E-Series Tarocchi</td>\n",
       "      <td>Philosophy, plate 28 from Arts and Sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21550</td>\n",
       "      <td>40694d77-c9d7-d861-e201-4228e99316e7</td>\n",
       "      <td>A work made of lithograph in black on white wo...</td>\n",
       "      <td>A favorite of Daumier’s, this print is a play ...</td>\n",
       "      <td>A favorite of Daumier’s, this print is a play ...</td>\n",
       "      <td>Honoré-Victorin Daumier</td>\n",
       "      <td>Sight, plate 39 from Types Parisiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25374</td>\n",
       "      <td>329b85e4-2865-1281-feeb-5c0ab47e500e</td>\n",
       "      <td>A work made of etching in black on ivory laid ...</td>\n",
       "      <td>This etching might represent two related prove...</td>\n",
       "      <td>This etching might represent two related prove...</td>\n",
       "      <td>Pieter Bruegel, the elder</td>\n",
       "      <td>The Hare Hunters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   art_id                              image_id  \\\n",
       "0    8101  c24feb4e-d7f5-791e-58ee-5db1a40d0a0d   \n",
       "1   11460  b84f047e-f871-48c5-6bbf-618731650105   \n",
       "2   21550  40694d77-c9d7-d861-e201-4228e99316e7   \n",
       "3   25374  329b85e4-2865-1281-feeb-5c0ab47e500e   \n",
       "\n",
       "                                            alt_text  \\\n",
       "0  A work made of cotton, plain weave; drawnwork ...   \n",
       "1  A work made of engraving in black on ivory lai...   \n",
       "2  A work made of lithograph in black on white wo...   \n",
       "3  A work made of etching in black on ivory laid ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  This apron mimics lace in its play of transluc...   \n",
       "1  The ', <a href='https://www.artic.edu/artists/...   \n",
       "2  A favorite of Daumier’s, this print is a play ...   \n",
       "3  This etching might represent two related prove...   \n",
       "\n",
       "                                    full_description  \\\n",
       "0  This apron mimics lace in its play of transluc...   \n",
       "1  The ', <a href='https://www.artic.edu/artists/...   \n",
       "2  A favorite of Daumier’s, this print is a play ...   \n",
       "3  This etching might represent two related prove...   \n",
       "\n",
       "                            artist  \\\n",
       "0                              NaN   \n",
       "1  Master of the E-Series Tarocchi   \n",
       "2          Honoré-Victorin Daumier   \n",
       "3        Pieter Bruegel, the elder   \n",
       "\n",
       "                                         title  \n",
       "0                                        Apron  \n",
       "1  Philosophy, plate 28 from Arts and Sciences  \n",
       "2         Sight, plate 39 from Types Parisiens  \n",
       "3                             The Hare Hunters  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_df = pd.read_csv(os.path.join(DATA_DIR, \"artworks.csv\"))\n",
    "art_df = art_df.drop_duplicates(subset=[\"description\", \"image_id\"])\n",
    "art_df = art_df.dropna(subset=[\"description\"])\n",
    "art_df = art_df.reset_index(drop=True)\n",
    "art_df = art_df.loc[art_df[\"image_id\"].isin(image_ids)]\n",
    "art_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "742f426c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "742f426c",
    "outputId": "c93c011d-b92e-4bde-f0aa-617d4646c044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10468 entries, 0 to 10467\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   art_id            10468 non-null  int64 \n",
      " 1   image_id          10468 non-null  object\n",
      " 2   alt_text          10468 non-null  object\n",
      " 3   description       10468 non-null  object\n",
      " 4   full_description  10468 non-null  object\n",
      " 5   artist            9871 non-null   object\n",
      " 6   title             10468 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 654.2+ KB\n"
     ]
    }
   ],
   "source": [
    "art_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2cc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = art_df[\"description\"].to_list()\n",
    "imgs_list = art_df[\"image_id\"].apply(lambda x: os.path.join(ARTIC_IMAGE_DIR, f\"{x}.jpg\")).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bf604e",
   "metadata": {
    "id": "f4bf604e"
   },
   "outputs": [],
   "source": [
    "train_images, val_images, train_texts, val_texts = train_test_split(\n",
    "    imgs_list, text_list, test_size=0.2, random_state=17\n",
    ")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "train_tokens = tokenizer(\n",
    "    train_texts,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"tf\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "val_tokens = tokenizer(\n",
    "    val_texts,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"tf\",\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80cb9527",
   "metadata": {
    "id": "80cb9527"
   },
   "outputs": [],
   "source": [
    "def get_img_emb(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE], method=\"nearest\")\n",
    "    image = tf.transpose(image, [2, 0, 1])  # Channels first\n",
    "    return image\n",
    "\n",
    "\n",
    "def data_mapper(img, input_ids, attention_mask):\n",
    "    return get_img_emb(img), tf.squeeze(input_ids), tf.squeeze(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0219d4a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0219d4a1",
    "outputId": "deec3a16-5579-461f-c7db-cfbd485db326",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Shapes\n",
      "Image: (64, 3, 224, 224)\n",
      "Input Id: (64, 77)\n",
      "Attention Mask: (64, 77)\n",
      "\n",
      "\n",
      "Val Dataset Shapes\n",
      "Image: (64, 3, 224, 224)\n",
      "Input Id: (64, 77)\n",
      "Attention Mask: (64, 77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 01:06:56.598818: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-08-28 01:06:56.722840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_images, train_tokens[\"input_ids\"], train_tokens[\"attention_mask\"]))\n",
    "    .map(data_mapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(2000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(\"Train Dataset Shapes\")\n",
    "for i in train_ds.take(1):\n",
    "    for nm, tensor in zip([\"Image\", \"Input Id\", \"Attention Mask\"], i):\n",
    "        print(f\"{nm}: {tensor.shape}\")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_images, val_tokens[\"input_ids\"], val_tokens[\"attention_mask\"]))\n",
    "    .map(data_mapper, num_parallel_calls=N_CPU)\n",
    "    .cache()\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Val Dataset Shapes\")\n",
    "for i in val_ds.take(1):\n",
    "    for nm, tensor in zip([\"Image\", \"Input Id\", \"Attention Mask\"], i):\n",
    "        print(f\"{nm}: {tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6a7e9b9",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09afa4b2",
   "metadata": {},
   "source": [
    "### Loading CLIP Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96527c38",
   "metadata": {},
   "source": [
    "Our goal is to obtain embeddings of images and texts that enable us to accurately measure the similarity between an image and its corresponding description. To achieve this, we must first obtain the image and text embeddings. We can do this by obtaining the projection layer weights of the vision and text encoder models of CLIP, which will provide us with a solid foundation for measuring the distance between images and their associated descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d150f97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d150f97",
    "outputId": "a6c85b0b-85ae-4cd6-da0f-255095256b5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_weights = tf.Variable(model.weights[-2])\n",
    "text_weights = tf.Variable(model.weights[-1])\n",
    "\n",
    "del model\n",
    "# Clear the Keras backend now that we deleted the original model.\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d8afe43",
   "metadata": {},
   "source": [
    "### Load Pretrained CLIPTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "579405e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "579405e4",
    "outputId": "8be7c5bf-a60b-4bae-e7b2-4a3883229afe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing TFCLIPTextModel: ['clip/vision_model/encoder/layers_._8/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._9/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._2/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._11/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._6/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._6/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._7/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._0/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._5/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._2/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._1/self_attn/v_proj/kernel:0', 'clip/vision_model/embeddings/class_embedding:0', 'clip/vision_model/encoder/layers_._1/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._8/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._3/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._6/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._2/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._5/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._8/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._7/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._1/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._10/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._0/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._5/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._5/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._7/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._2/self_attn/v_proj/kernel:0', 'clip/vision_model/embeddings/patch_embedding/kernel:0', 'clip/vision_model/pre_layrnorm/beta:0', 'clip/vision_model/encoder/layers_._3/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._5/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._3/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._3/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._9/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._5/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._7/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._11/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._0/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._2/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._8/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._9/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._11/layer_norm2/beta:0', 'clip/vision_model/embeddings/position_embedding/embeddings:0', 'clip/vision_model/encoder/layers_._11/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._4/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._0/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._9/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._4/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._9/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._5/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._9/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._0/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._5/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._3/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._1/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._1/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._1/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._8/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._2/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._3/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._10/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._10/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._2/mlp/fc2/bias:0', 'clip/text_projection/kernel:0', 'clip/vision_model/encoder/layers_._0/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._5/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._0/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/self_attn/q_proj/bias:0', 'clip/logit_scale:0', 'clip/vision_model/encoder/layers_._10/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._2/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._2/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._7/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._4/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._2/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._2/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._4/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._7/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._11/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._7/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._4/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._6/self_attn/v_proj/bias:0', 'clip/vision_model/pre_layrnorm/gamma:0', 'clip/vision_model/encoder/layers_._11/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._2/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._5/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._10/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._0/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._3/self_attn/q_proj/bias:0', 'clip/vision_model/post_layernorm/beta:0', 'clip/vision_model/encoder/layers_._11/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._8/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._0/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._6/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._5/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._1/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._4/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._0/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._3/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._1/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._6/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._4/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._0/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._1/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._1/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._3/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._8/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._0/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._11/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._1/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._7/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._6/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._10/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._3/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._4/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._3/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._8/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._4/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._7/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._11/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._11/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._10/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._1/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._9/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._9/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._6/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._1/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._5/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._10/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._8/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._7/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._5/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._10/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._9/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._2/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._2/layer_norm1/gamma:0', 'clip/visual_projection/kernel:0', 'clip/vision_model/encoder/layers_._1/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._2/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._5/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._7/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._2/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._5/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._4/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._5/mlp/fc2/bias:0', 'clip/vision_model/post_layernorm/gamma:0', 'clip/vision_model/encoder/layers_._9/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._3/mlp/fc1/bias:0']\n",
      "- This IS expected if you are initializing TFCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFCLIPTextModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPTextModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "CLIP_text_model = TFCLIPTextModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ca68279",
   "metadata": {},
   "source": [
    "### Load Pretrained CLIPVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "448655c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "448655c3",
    "outputId": "928eb612-e96d-42b3-93b2-c72e20988c6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing TFCLIPVisionModel: ['clip/text_model/encoder/layers_._5/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/bias:0', 'clip/text_model/embeddings/position_embedding/embeddings:0', 'clip/text_model/encoder/layers_._2/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._4/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._11/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._4/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._4/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/beta:0', 'clip/text_model/final_layer_norm/beta:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/bias:0', 'clip/text_projection/kernel:0', 'clip/logit_scale:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._6/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._1/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._10/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._5/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._6/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._6/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._0/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._9/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._7/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._7/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._5/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm2/gamma:0', 'clip/text_model/embeddings/token_embedding/weight:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/bias:0', 'clip/text_model/final_layer_norm/gamma:0', 'clip/text_model/encoder/layers_._7/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._0/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._3/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._0/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._9/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/layer_norm2/gamma:0', 'clip/visual_projection/kernel:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._11/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._3/layer_norm1/gamma:0']\n",
      "- This IS expected if you are initializing TFCLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFCLIPVisionModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "CLIP_vision_model = TFCLIPVisionModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c7f1475",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c7f1475",
    "outputId": "6063b919-87d6-4566-d2b5-af904d0463bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfclip_vision_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " clip (TFCLIPVisionMainLayer  multiple                 87456000  \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,456,000\n",
      "Trainable params: 87,456,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Model: \"tfclip_text_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " clip (TFCLIPTextMainLayer)  multiple                  63165952  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,165,952\n",
      "Trainable params: 63,165,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# NOTE: If you have GPU restrictions you can set the base models as non-trainable\n",
    "# and only train the projector layer for a faster optimization.\n",
    "# CLIP_vision_model.trainable = False\n",
    "# CLIP_text_model.trainable = False\n",
    "\n",
    "CLIP_vision_model.summary()\n",
    "print(\"\\n\")\n",
    "CLIP_text_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8b38d13",
   "metadata": {},
   "source": [
    "## Joint Embedding Projections\n",
    "\n",
    "Because the embedding sizes of CLIPVisionModel and CLIPTextModel are different, we'll need to add projection layers to the top of both models to ensure that their embeddings are of the same dimension. This step is essential for successful integration of the models, as the models require embeddings to be of the same dimension to be processed together. The projection layers help to transform the embeddings from one dimension to another, allowing for both the text and the images to be jointly embedded in the same space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82f9f3f5",
   "metadata": {
    "id": "82f9f3f5"
   },
   "outputs": [],
   "source": [
    "def get_image_model(n_dims=512):\n",
    "    x = inputs = tf.keras.layers.Input((COLOR_CHANNELS, IMG_SIZE, IMG_SIZE), name=\"image\")\n",
    "    x = CLIP_vision_model(x).pooler_output  # pooled CLS states\n",
    "    kernel_weights = tf.constant_initializer(vision_weights.numpy())\n",
    "    # Projection layer\n",
    "    embed = tf.keras.layers.Dense(n_dims, name=\"image_embedding\", kernel_initializer=kernel_weights)(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=embed, name=\"image_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c0e31cb",
   "metadata": {
    "id": "0c0e31cb"
   },
   "outputs": [],
   "source": [
    "def get_text_model(n_dims=512):\n",
    "    inputs1 = tf.keras.layers.Input((N_TOKENS), dtype=tf.int32, name=\"input_ids\")\n",
    "    inputs2 = tf.keras.layers.Input((N_TOKENS), dtype=tf.int32, name=\"attention_mask\")\n",
    "    x = CLIP_text_model(input_ids=inputs1, attention_mask=inputs2).pooler_output  # pooled CLS states\n",
    "    kernel_weights = tf.constant_initializer(text_weights.numpy())\n",
    "    # Projection layer\n",
    "    embed = tf.keras.layers.Dense(n_dims, name=\"text_embedding\", kernel_initializer=kernel_weights)(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=embed, name=\"text_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0af467a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0af467a",
    "outputId": "91cd68a2-49f8-4775-999d-078a1264c54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"image_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image (InputLayer)          [(None, 3, 224, 224)]     0         \n",
      "                                                                 \n",
      " tfclip_vision_model (TFCLIP  TFBaseModelOutputWithPoo  87456000 \n",
      " VisionModel)                ling(last_hidden_state=(            \n",
      "                             None, 50, 768),                     \n",
      "                              pooler_output=(None, 76            \n",
      "                             8),                                 \n",
      "                              hidden_states=None, att            \n",
      "                             entions=None)                       \n",
      "                                                                 \n",
      " image_embedding (Dense)     (None, 512)               393728    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,849,728\n",
      "Trainable params: 87,849,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Model: \"text_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 77)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 77)]         0           []                               \n",
      "                                                                                                  \n",
      " tfclip_text_model (TFCLIPTextM  TFBaseModelOutputWi  63165952   ['input_ids[0][0]',              \n",
      " odel)                          thPooling(last_hidd               'attention_mask[0][0]']         \n",
      "                                en_state=(None, 77,                                               \n",
      "                                 512),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 512),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " text_embedding (Dense)         (None, 512)          262656      ['tfclip_text_model[0][1]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 63,428,608\n",
      "Trainable params: 63,428,608\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_model = get_image_model()\n",
    "text_model = get_text_model()\n",
    "\n",
    "img_model.summary()\n",
    "print(\"\\n\")\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1860e54",
   "metadata": {
    "id": "c1860e54"
   },
   "source": [
    "### Losses and Metrics\n",
    "\n",
    "We have a dataset which have pairs of image and text. We only have positive examples here and in such a case multiple negatives ranking loss is a suitable choice. This considers every other pair other than ($x_i$, $y_i$) as negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60148171",
   "metadata": {
    "id": "c1860e54"
   },
   "outputs": [],
   "source": [
    "loss_fn = tfsim.losses.MultiNegativesRankLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wI5uQIC8xG4z",
   "metadata": {
    "id": "wI5uQIC8xG4z"
   },
   "source": [
    "Let's measure the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "998018f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "998018f7",
    "outputId": "aa6b4c15-8105-4454-aef5-b096b843841a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:06<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation Loss: 2.7631527149316035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss = 0\n",
    "base_image_embeddings = []\n",
    "base_text_embeddings = []\n",
    "for image_batch, input_ids_batch, attention_mask_batch in tqdm(val_ds):\n",
    "    image_embedding = img_model(image_batch, training=False)\n",
    "    text_embedding = text_model([input_ids_batch, attention_mask_batch], training=False)\n",
    "\n",
    "    image_embedding = tf.math.l2_normalize(image_embedding, axis=1)\n",
    "    text_embedding = tf.math.l2_normalize(text_embedding, axis=1)\n",
    "\n",
    "    base_image_embeddings.append(image_embedding.numpy())\n",
    "    base_text_embeddings.append(text_embedding.numpy())\n",
    "\n",
    "    # Compute the loss value for this minibatch.\n",
    "    loss_value = loss_fn(text_embedding, image_embedding)\n",
    "    val_loss += float(loss_value)\n",
    "\n",
    "print(f\"Mean Validation Loss: {val_loss / len(val_ds)}\")\n",
    "base_image_embeddings = np.concatenate(base_image_embeddings)\n",
    "base_text_embeddings = np.concatenate(base_text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b6183",
   "metadata": {
    "id": "877b6183"
   },
   "source": [
    "We'll use R@k metric for evaluation. This is a common metric used in evaluation of ranking. It is the average of recall by taking top k predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5da6f8f",
   "metadata": {
    "id": "a5da6f8f"
   },
   "outputs": [],
   "source": [
    "def recall_at_k(sim_matrix, k=1):\n",
    "    \"\"\"\n",
    "    It is the mean of ratio of correctly retrieved documents\n",
    "    to the number of relevant documents.\n",
    "    This implementation is specific to\n",
    "    data having unique label for each key\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_mat = np.argsort(sim_matrix, axis=1)[:, -k:]\n",
    "\n",
    "    # Each key has unique label\n",
    "    true_labels = np.arange(sorted_mat.shape[0]).reshape(-1, 1)\n",
    "    true_labels = np.repeat(true_labels, k, axis=1)\n",
    "    sorted_mat = sorted_mat - true_labels\n",
    "    # the position in row corresponding to true positive\n",
    "    # will be zero\n",
    "    tps = np.any(sorted_mat == 0, axis=1)\n",
    "    return tps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82b1b414",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82b1b414",
    "outputId": "fce48db4-ce50-4a7c-b603-738ad7a8b1b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@1: 0.1361031518624642\n",
      "R@2: 0.18958930276981853\n",
      "R@3: 0.22015281757402103\n",
      "R@4: 0.24880611270296085\n",
      "R@5: 0.27650429799426934\n"
     ]
    }
   ],
   "source": [
    "base_sim_mat = np.matmul(base_text_embeddings, base_image_embeddings.T)\n",
    "for k in range(1, 6):\n",
    "    print(\"R@{}: {}\".format(k, recall_at_k(base_sim_mat, k)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "GFPy9nhaehJT",
   "metadata": {
    "id": "GFPy9nhaehJT"
   },
   "source": [
    "## Training\n",
    "\n",
    "To train our models, we need to use a custom training loop that can handle the training of two models simultaneously. We can follow the approach outlined in [this guide](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#end-to-end_example_a_gan_training_loop_from_scratch), which shows us how to create a custom training loop from scratch using Keras.\n",
    "\n",
    "We will define the train_step, which updates the models' weights based on a batch of data, and then initiate the training loop. During each epoch, we will call the train_step for every batch, allowing our models to gradually learn from the data and improve their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "851709f1",
   "metadata": {
    "id": "851709f1"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "img_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "text_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "train_step_losses = []\n",
    "train_epoch_losses = []\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(image_batch, text_batch):\n",
    "    with tf.GradientTape() as img_tape, tf.GradientTape() as text_tape:\n",
    "        import pdb; pdb.set_trace()\n",
    "        image_embedding = img_model(image_batch, training=True)\n",
    "        text_embedding = text_model(text_batch, training=True)\n",
    "\n",
    "        image_embedding = tf.math.l2_normalize(image_embedding, axis=1)\n",
    "        text_embedding = tf.math.l2_normalize(text_embedding, axis=1)\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss_fn(text_embedding, image_embedding)\n",
    "\n",
    "    img_grads = img_tape.gradient(loss_value, img_model.trainable_weights)\n",
    "    text_grads = text_tape.gradient(loss_value, text_model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    img_optimizer.apply_gradients(zip(img_grads, img_model.trainable_weights))\n",
    "    text_optimizer.apply_gradients(zip(text_grads, text_model.trainable_weights))\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fa53101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fa53101",
    "outputId": "8be0e0cd-9212-4f1c-ae63-c886f0b00c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "> \u001b[0;32m/tmp/ipykernel_40274/2620194056.py\u001b[0m(13)\u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m      9 \u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     10 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     11 \u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     12 \u001b[0m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     14 \u001b[0m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     15 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     16 \u001b[0m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     17 \u001b[0m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     18 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     19 \u001b[0m        \u001b[0;31m# Compute the loss value for this minibatch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     20 \u001b[0m        \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     22 \u001b[0m    \u001b[0mimg_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     23 \u001b[0m    \u001b[0mtext_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     24 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     25 \u001b[0m    \u001b[0;31m# Run one step of gradient descent by updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     26 \u001b[0m    \u001b[0;31m# the value of the variables to minimize the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     27 \u001b[0m    \u001b[0mimg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     28 \u001b[0m    \u001b[0mtext_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     29 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     30 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 01:15:37.878150: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 37.50MiB (rounded to 39321600)requested by op Sigmoid\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-08-28 01:15:37.878812: W tensorflow/tsl/framework/bfc_allocator.cc:497] ***************************************************************************************x************\n",
      "2023-08-28 01:15:37.878851: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'mlp' (type TFCLIPMLP).\n",
      "\n",
      "{{function_node __wrapped__Sigmoid_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Sigmoid]\n",
      "\n",
      "Call arguments received by layer 'mlp' (type TFCLIPMLP):\n",
      "  • hidden_states=tf.Tensor(shape=(64, 50, 768), dtype=float32)\n",
      "> \u001b[0;32m/tmp/ipykernel_40274/2620194056.py\u001b[0m(13)\u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m      9 \u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     10 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     11 \u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     12 \u001b[0m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     14 \u001b[0m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     15 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     16 \u001b[0m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     17 \u001b[0m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     18 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     19 \u001b[0m        \u001b[0;31m# Compute the loss value for this minibatch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     20 \u001b[0m        \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     22 \u001b[0m    \u001b[0mimg_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     23 \u001b[0m    \u001b[0mtext_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     24 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     25 \u001b[0m    \u001b[0;31m# Run one step of gradient descent by updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     26 \u001b[0m    \u001b[0;31m# the value of the variables to minimize the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     27 \u001b[0m    \u001b[0mimg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     28 \u001b[0m    \u001b[0mtext_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     29 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     30 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  image_embedding.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([46, 512])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  image_batch.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([64, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_40274/2620194056.py\u001b[0m(11)\u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m\u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_40274/2620194056.py\u001b[0m(11)\u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m\u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/tmp/ipykernel_40274/2620194056.py\u001b[0m(13)\u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m        \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'mlp' (type TFCLIPMLP).\n",
      "\n",
      "{{function_node __wrapped__Sigmoid_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Sigmoid]\n",
      "\n",
      "Call arguments received by layer 'mlp' (type TFCLIPMLP):\n",
      "  • hidden_states=tf.Tensor(shape=(64, 50, 768), dtype=float32)\n",
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m(871)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    869 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_functions_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    870 \u001b[0;31m      \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_function_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 871 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    872 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    873 \u001b[0;31m    \u001b[0;31m# Only count the statistics the first time, before initialization took\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m(870)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    868 \u001b[0;31m    \u001b[0;31m# Implements GenericFunction.__call__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    869 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_functions_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 870 \u001b[0;31m      \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_function_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    871 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    872 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m(871)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    869 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_functions_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    870 \u001b[0;31m      \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_function_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 871 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    872 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    873 \u001b[0;31m    \u001b[0;31m# Only count the statistics the first time, before initialization took\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'mlp' (type TFCLIPMLP).\n",
      "\n",
      "{{function_node __wrapped__Sigmoid_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Sigmoid]\n",
      "\n",
      "Call arguments received by layer 'mlp' (type TFCLIPMLP):\n",
      "  • hidden_states=tf.Tensor(shape=(64, 50, 768), dtype=float32)\n",
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m(150)\u001b[0;36merror_handler\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    148 \u001b[0;31m    \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    149 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 150 \u001b[0;31m      \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    152 \u001b[0;31m      \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m(151)\u001b[0;36merror_handler\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    149 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m      \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 151 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    152 \u001b[0;31m      \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    153 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m(152)\u001b[0;36merror_handler\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    150 \u001b[0;31m      \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 152 \u001b[0;31m      \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    153 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    154 \u001b[0;31m    \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m(153)\u001b[0;36merror_handler\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    151 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    152 \u001b[0;31m      \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 153 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    154 \u001b[0;31m    \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    155 \u001b[0;31m      \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'mlp' (type TFCLIPMLP).\n",
      "\n",
      "{{function_node __wrapped__Sigmoid_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Sigmoid]\n",
      "\n",
      "Call arguments received by layer 'mlp' (type TFCLIPMLP):\n",
      "  • hidden_states=tf.Tensor(shape=(64, 50, 768), dtype=float32)\n",
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m(153)\u001b[0;36merror_handler\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    151 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    152 \u001b[0;31m      \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 153 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    154 \u001b[0;31m    \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    155 \u001b[0;31m      \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m(155)\u001b[0;36merror_handler\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    153 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    154 \u001b[0;31m    \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 155 \u001b[0;31m      \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    156 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    157 \u001b[0;31m  \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 01:16:39.842105: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Custom Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (image_batch, input_ids_batch, attention_mask_batch) in enumerate(train_ds):\n",
    "        loss_value = train_step(image_batch, [input_ids_batch, attention_mask_batch])\n",
    "        epoch_loss += float(loss_value)\n",
    "        train_step_losses.append(float(loss_value) / image_batch.shape[0])\n",
    "        # Log every batch\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step + 1}: {float(loss_value):.4f}\")\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * BATCH_SIZE))\n",
    "\n",
    "    print(f\"Epoch loss: {epoch_loss / len(train_ds)}\")\n",
    "    train_epoch_losses.append(epoch_loss / len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ceaa3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "ec7ceaa3",
    "outputId": "e5e056ce-e8c0-42b2-edb0-91f7d9eb103f"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_epoch_losses)\n",
    "plt.title(\"Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09ad3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "0f09ad3a",
    "outputId": "2b811f7d-c964-4a34-88fc-16fb11eba518"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_step_losses)\n",
    "plt.title(\"Training\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YBl3IQe5t8Dt",
   "metadata": {
    "id": "YBl3IQe5t8Dt"
   },
   "source": [
    "Now let's check the performance on validation data after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c74dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a32c74dc",
    "outputId": "e932916f-d0ca-4b21-8823-1ec7533afd0a"
   },
   "outputs": [],
   "source": [
    "val_loss = 0\n",
    "image_embeddings = []\n",
    "text_embeddings = []\n",
    "for image_batch, input_ids_batch, attention_mask_batch in tqdm(val_ds):\n",
    "    image_embedding = img_model(image_batch, training=False)\n",
    "    text_embedding = text_model([input_ids_batch, attention_mask_batch], training=False)\n",
    "\n",
    "    image_embedding = tf.math.l2_normalize(image_embedding, axis=1)\n",
    "    text_embedding = tf.math.l2_normalize(text_embedding, axis=1)\n",
    "\n",
    "    image_embeddings.append(image_embedding.numpy())\n",
    "    text_embeddings.append(text_embedding.numpy())\n",
    "\n",
    "    # Compute the loss value for this minibatch.\n",
    "    loss_value = loss_fn(text_embedding, image_embedding)\n",
    "    val_loss += float(loss_value)\n",
    "    \n",
    "print(f\"Mean Validation Loss: {val_loss / len(val_ds)}\")\n",
    "image_embeddings = np.concatenate(image_embeddings)\n",
    "text_embeddings = np.concatenate(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60953d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c60953d",
    "outputId": "373e2f79-2d9c-4c63-f999-dcd08b93b89c"
   },
   "outputs": [],
   "source": [
    "finetuned_sim = np.matmul(text_embeddings, image_embeddings.T)\n",
    "\n",
    "for k in range(1, 6):\n",
    "    print(\"R@{} : {}\".format(k, recall_at_k(finetuned_sim, k)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cd34761",
   "metadata": {},
   "source": [
    "## Search Using TF Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8xwd13weIVP",
   "metadata": {
    "id": "a8xwd13weIVP"
   },
   "source": [
    "We can see that thre is a significant imrovement in recall metric after only 5 epochs of training.\n",
    "Now, we can use these modes for painting retrival based on text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = tf.convert_to_tensor(np.array([get_img_emb(fp) for fp in imgs_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305657e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = tfsim.models.SimilarityModel(img_model.inputs, img_model.outputs)\n",
    "brute_force_search = tfsim.search.NMSLibSearch(\n",
    "    distance='cosine',\n",
    "    method='brute_force', \n",
    "    dim=img_model.output_shape[1]\n",
    ")\n",
    "image_index.create_index(search=brute_force_search)\n",
    "\n",
    "image_index.index(imgs, data=[{\"imgs\": i, \"desc\": d} for i,d in zip(imgs, text_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"Large waves crashing in the Japanese ukioe style\"\n",
    "\n",
    "query_tokens = tokenizer(\n",
    "    [SEARCH_QUERY],\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"tf\",\n",
    "    truncation=True,\n",
    ")\n",
    "q_input_ids = tf.convert_to_tensor(np.array(query_tokens[\"input_ids\"]))\n",
    "q_attention_mask = tf.convert_to_tensor(np.array(query_tokens[\"attention_mask\"]))\n",
    "query_emb = text_model.predict([q_input_ids, q_attention_mask])\n",
    "\n",
    "\n",
    "lookups = image_index._index.batch_lookup(predictions=query_emb, k=5, verbose=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c3978",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(lookups)):\n",
    "    matching_img = lookups[i].data[\"imgs\"]\n",
    "    matching_desc = lookups[i].data[\"desc\"]\n",
    "    matching_distance = lookups[i].distance\n",
    "\n",
    "    print(f\"Distance between text query and image: {matching_distance}\\n\")\n",
    "    print('\\n'.join(textwrap.wrap(matching_desc, width=120)))\n",
    "    # Channel order is C W H and we need to roll the axis to make it W H C\n",
    "    plt.imshow(np.rollaxis(matching_img.numpy(), 0, 3));\n",
    "    plt.show()\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa370eb1",
   "metadata": {
    "id": "aa370eb1"
   },
   "source": [
    "## Model Saving\n",
    "The models can be saved in the same way as any other keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6717ca",
   "metadata": {
    "id": "7f6717ca"
   },
   "outputs": [],
   "source": [
    "img_model.save(\"image_model.h5\")\n",
    "text_model.save(\"text_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2e778",
   "metadata": {
    "id": "a2d2e778"
   },
   "source": [
    "However when loading these saved models, the CLIP vision and text models must be provided in custom objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb4c52",
   "metadata": {
    "id": "e0fb4c52"
   },
   "outputs": [],
   "source": [
    "img_model = tf.keras.models.load_model(\"image_model.h5\", custom_objects={\"TFCLIPVisionModel\": TFCLIPVisionModel})\n",
    "text_model = tf.keras.models.load_model(\"text_model.h5\", custom_objects={\"TFCLIPTextModel\": TFCLIPTextModel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc1bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
