{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 The TensorFlow Similarity Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Similarity Self-Supervised Learning Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/similarity/blob/master/examples/supervised_visualization.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/similarity/blob/master/examples/supervised_visualization.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow Similarity](https://github.com/tensorflow/similarity) is a python package focused on making similarity learning quick and easy. \n",
    "\n",
    "### Notebook goal\n",
    "\n",
    "This notebook demonstrates how to use TensorFlow Similarity to pre-train a `ContrastiveModel()` on the cifar100 dataset.\n",
    "\n",
    "You are going to learn the main features offered by the `ContrastiveModel()` and will:\n",
    "\n",
    "1. Create a `SingleShotMemorySampler()` that will generate two augmented views for each example in a batch.\n",
    "\n",
    "2. Create basic versions of **Backbone**, **Projector**, and **Predictor** networks. These will be used to construct the `ContrastiveModel()`\n",
    "\n",
    "3. Build a `ContrastiveModel()` using one of the three supported algorithms.\n",
    "\n",
    "4. Compare the classification performance of training from scratch vs. using the frozen pre-trainined weights.\n",
    "\n",
    "### Things to try\n",
    "\n",
    "You can try the following things to improve the model performance:\n",
    "- Try different self-supervised learning algorithms.\n",
    "- Use a larger backbone model.\n",
    "- Use a larger embedding by increasing the size of the projection and predictor layers.\n",
    "- Try using other augmention functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# INFO messages are not printed.\n",
    "# This must be run before loading other modules.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "# install TF similarity if needed\n",
    "try:\n",
    "    import tensorflow_similarity as tfsim  # main package\n",
    "except ModuleNotFoundError:\n",
    "    !pip install tensorflow_similarity\n",
    "    import tensorflow_similarity as tfsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install TF addons if needed\n",
    "try:\n",
    "    import tensorflow_addons as tfa  # main package\n",
    "except ModuleNotFoundError:\n",
    "    !pip install tensorflow-addons\n",
    "    import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfsim.utils.tf_cap_memory()  # Avoid GPU memory blow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out any old model state.\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.7.0\n",
      "TensorFlow Similarity 0.15.0.dev65\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"TensorFlow Similarity\", tfsim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preperation\n",
    "\n",
    "We are going to load the CIFAR100 dataset and partition the data into the following splits:\n",
    "* **Train**: Data used for the pre-training and evaluation training phases.\n",
    "* **Validation** Data used for validation metrics during the pre-training phase. \n",
    "* **Test** Data reserved for the evaluation tests.\n",
    "* **Query and Index** Data used to compute matching metrics. The query data is used to retrieve the nearest indexed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../../kaggle_ds/google_landmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    }
   ],
   "source": [
    "((x_raw_train, y_raw_train), (x_test, y_test)), ds_info = tfds.load(\n",
    "    \"cifar10\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    batch_size=-1,\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Class Names: {ds_info.features['label'].names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the indicies for query, index, val, and train splits\n",
    "query_idxs, index_idxs, val_idxs, train_idxs = [], [], [], []\n",
    "for cid in range(ds_info.features[\"label\"].num_classes):\n",
    "    idxs = tf.random.shuffle(tf.where(y_raw_train == cid))\n",
    "    idxs = tf.reshape(idxs, (-1,))\n",
    "    query_idxs.extend(idxs[:200])  # 200 query examples per class\n",
    "    index_idxs.extend(idxs[200:400])  # 200 index examples per class\n",
    "    val_idxs.extend(idxs[400:500])  # 100 validation examples per class\n",
    "    train_idxs.extend(idxs[500:])  # The remaining are used for training\n",
    "\n",
    "random.shuffle(query_idxs)\n",
    "random.shuffle(index_idxs)\n",
    "random.shuffle(val_idxs)\n",
    "random.shuffle(train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(idxs: list) -> tuple:\n",
    "    x, y = [], []\n",
    "    for idx in idxs:\n",
    "        x.append(x_raw_train[int(idx)])\n",
    "        y.append(y_raw_train[int(idx)])\n",
    "    return tf.convert_to_tensor(x), tf.convert_to_tensor(y)\n",
    "\n",
    "\n",
    "x_query, y_query = create_split(query_idxs)\n",
    "x_index, y_index = create_split(index_idxs)\n",
    "x_val, y_val = create_split(val_idxs)\n",
    "x_train, y_train = create_split(train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tabulate(\n",
    "        [\n",
    "            [\"train\", x_train.shape, y_train.shape],\n",
    "            [\"val\", x_val.shape, y_val.shape],\n",
    "            [\"test\", x_test.shape, y_test.shape],\n",
    "            [\"query\", x_query.shape, y_query.shape],\n",
    "            [\"index\", x_index.shape, y_index.shape],\n",
    "        ],\n",
    "        headers=[\"Examples\", \"Labels\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM = \"simsiam\"  # @param [\"barlow\", \"simsiam\", \"simclr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_IMG_SIZE = 32\n",
    "\n",
    "if ALGORITHM == \"simsiam\":\n",
    "    BATCH_SIZE = 512\n",
    "    PRE_TRAIN_EPOCHS = 800\n",
    "    PRE_TRAIN_STEPS_PER_EPOCH = len(x_train) // BATCH_SIZE\n",
    "    VAL_STEPS_PER_EPOCH = 20\n",
    "    WEIGHT_DECAY = 5e-4\n",
    "    INIT_LR = 3e-2 * int(BATCH_SIZE / 256)\n",
    "    WARMUP_LR = 0.0\n",
    "    WARMUP_STEPS = 0\n",
    "    DIM = 2048  # The layer size for the projector and predictor models.\n",
    "elif ALGORITHM == \"barlow\":\n",
    "    BATCH_SIZE = 512\n",
    "    PRE_TRAIN_EPOCHS = 100\n",
    "    PRE_TRAIN_STEPS_PER_EPOCH = len(x_train) // BATCH_SIZE\n",
    "    VAL_STEPS_PER_EPOCH = 20\n",
    "    WEIGHT_DECAY = 5e-4\n",
    "    INIT_LR = 1e-3  # Initial LR for the learning rate schedule.\n",
    "    WARMUP_LR = 0.0\n",
    "    WARMUP_STEPS = 1000  # we have 10k train steps and we warm up over 10% of that.\n",
    "    DIM = 2048  # The layer size for the projector and predictor models.\n",
    "elif ALGORITHM == \"simclr\":\n",
    "    BATCH_SIZE = 512\n",
    "    PRE_TRAIN_EPOCHS = 100\n",
    "    PRE_TRAIN_STEPS_PER_EPOCH = len(x_train) // BATCH_SIZE\n",
    "    VAL_STEPS_PER_EPOCH = 20\n",
    "    WEIGHT_DECAY = 5e-4\n",
    "    INIT_LR = 1e-3  # Initial LR for the learning rate schedule.\n",
    "    WARMUP_LR = 0.0\n",
    "    WARMUP_STEPS = 0\n",
    "    DIM = 2048  # The layer size for the projector and predictor models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single Shot DataSet\n",
    "\n",
    "Self-supervised networks require two augmented \"views\" of each example. This can be created using a DataSet and an augmentation function. The DataSet treats each example in the batch as it's own class and then the augment function produces two separate views for each example. \n",
    "\n",
    "This means the resulting batch will yield tuples containing the two views, i.e., `Tuple[(BATCH_SIZE, 32, 32, 3), (BATCH_SIZE, 32, 32, 3)]`. \n",
    "\n",
    "TensorFlow Similarity provides several random augmentation functions, and here we are using the `SimCLRAugmenter()`. This augmenter returns values between [0, 1], but our ResNet backbone model expects the values to be between [0, 255] so we also scale the final output values in the augmentation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def simsiam_augmenter(img, blur=True, area_range=(0.2, 1.0)):\n",
    "    \"\"\"SimSiam augmenter.\n",
    "\n",
    "    The SimSiam augmentations are based on the SimCLR augmentations, but have\n",
    "    some important differences.\n",
    "    * The crop area lower bound is 20% instead of 8%.\n",
    "    * The color jitter and grayscale are applied separately instead of together.\n",
    "    * The color jitter ranges are much smaller.\n",
    "    * Blur is not applied for the cifar10 dataset.\n",
    "\n",
    "    args:\n",
    "        img: Single image tensor of shape (H, W, C)\n",
    "        blur: If true, apply blur. Should be disabled for cifar10.\n",
    "        area_range: The upper and lower bound of the random crop percentage.\n",
    "\n",
    "    returns:\n",
    "        A single image tensor of shape (H, W, C) with values between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    # random resize and crop. Increase the size before we crop.\n",
    "    img = tfsim.augmenters.simclr.crop_and_resize(\n",
    "        img, CIFAR_IMG_SIZE, CIFAR_IMG_SIZE, area_range=area_range\n",
    "    )\n",
    "    img /= 255.0\n",
    "    # random color jitter\n",
    "    def _jitter_transform(x):\n",
    "        return tfsim.augmenters.simclr.color_jitter_rand(\n",
    "            x,\n",
    "            np.random.uniform(0.0, 0.4),\n",
    "            np.random.uniform(0.0, 0.4),\n",
    "            np.random.uniform(0.0, 0.4),\n",
    "            np.random.uniform(0.0, 0.1),\n",
    "            \"simclrv2\",\n",
    "        )\n",
    "\n",
    "    img = tfsim.augmenters.simclr.random_apply(_jitter_transform, p=0.8, x=img)\n",
    "\n",
    "    # # random grayscale\n",
    "    def _grascayle_transform(x):\n",
    "        return tfsim.augmenters.simclr.to_grayscale(x)\n",
    "\n",
    "    img = tfsim.augmenters.simclr.random_apply(_grascayle_transform, p=0.2, x=img)\n",
    "\n",
    "    # optional random gaussian blur\n",
    "    if blur:\n",
    "        img = tfsim.augmenters.simclr.random_blur(img, p=0.5)\n",
    "\n",
    "    # random horizontal flip\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def process(img):\n",
    "    # SimClrAugmenter returns scaled values between [0,1]\n",
    "    view1 = simsiam_augmenter(img, blur=False)\n",
    "    view2 = simsiam_augmenter(img, blur=False)\n",
    "    # The ResNet model scales internally and expects the inputs to be between [0, 255]\n",
    "    return (view1 * 255.0, view2 * 255.0)\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = train_ds.shuffle(1024)\n",
    "train_ds = train_ds.map(process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(x_val)\n",
    "val_ds = val_ds.repeat()\n",
    "val_ds = val_ds.shuffle(1024)\n",
    "val_ds = val_ds.map(process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfsim.visualization.visualize_views(\n",
    "    views=next(train_ds.as_numpy_iterator()),\n",
    "    num_imgs=16,\n",
    "    views_per_col=8,\n",
    "    max_pixel_value=255.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models\n",
    "\n",
    "There are various architectures for building self-supervised models which may include some of the following:\n",
    "* **Backbone**: This is the base model and is typically an existing architecture like ResNet or EfficientNet.\n",
    "* **Projector**: This is a small multi-layer Neural Net and provides the embedding features at the end of training.\n",
    "* **Predictor**: This model is used by BYOL and SimSiam and provides an additional small multi-layer Neural Net.\n",
    "\n",
    "Typically, the projector and predictor networks are only 2 or 3 layers with batch normalization. Several papers claim that the batch normalization is critical to prevent the model from colapsing to a degenerate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Backbone Model\n",
    "\n",
    "Describe why we don't use ImageNet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(\n",
    "    img_size, activation=\"relu\", preproc_mode=\"torch\"\n",
    "):\n",
    "    input_shape = (img_size, img_size, 3)\n",
    "    backbone = tfsim.architectures.ResNet18Sim(\n",
    "        input_shape,\n",
    "        augmentation=None,\n",
    "        include_top=False,  # Take the pooling layer as the output.\n",
    "        pooling=\"avg\",  # Use GeneralizedMeanPooling2D\n",
    "        # gem_p=3.0,  # Increase the contrast between activations in the feature map.\n",
    "        preproc_mode=\"torch\",\n",
    "    )\n",
    "    return backbone\n",
    "\n",
    "\n",
    "backbone = get_backbone(CIFAR_IMG_SIZE)\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector(input_dim, dim, activation=\"relu\", num_layers: int = 3):\n",
    "    inputs = tf.keras.layers.Input((input_dim,), name=\"projector_input\")\n",
    "    x = inputs\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            dim,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=tf.keras.initializers.LecunUniform(),\n",
    "            name=f\"projector_layer_{i}\",\n",
    "        )(x)\n",
    "        x = tf.keras.layers.BatchNormalization(\n",
    "            epsilon=1.001e-5, name=f\"batch_normalization_{i}\"\n",
    "        )(x)\n",
    "        x = tf.keras.layers.Activation(activation, name=f\"{activation}_activation_{i}\")(\n",
    "            x\n",
    "        )\n",
    "    x = tf.keras.layers.Dense(\n",
    "        dim,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.keras.initializers.LecunUniform(),\n",
    "        name=\"projector_output\",\n",
    "    )(x)\n",
    "    x = tf.keras.layers.BatchNormalization(\n",
    "        epsilon=1.001e-5,\n",
    "        center=False,  # Page:5, Paragraph:2 of SimSiam paper\n",
    "        scale=False,  # Page:5, Paragraph:2 of SimSiam paper\n",
    "        name=f\"batch_normalization_ouput\",\n",
    "    )(x)\n",
    "    o = tfsim.layers.ActivationStdLoggingLayer(name=\"proj_std\")(x)\n",
    "    projector = tf.keras.Model(inputs, o, name=\"projector\")\n",
    "    return projector\n",
    "\n",
    "\n",
    "projector = get_projector(input_dim=backbone.output.shape[-1], dim=DIM, num_layers=2)\n",
    "projector.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictor(input_dim, hidden_dim=512, activation=\"relu\"):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim,), name=\"predictor_input\")\n",
    "    x = inputs\n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        hidden_dim,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.keras.initializers.LecunUniform(),\n",
    "        name=\"predictor_layer_0\",\n",
    "    )(x)\n",
    "    x = tf.keras.layers.BatchNormalization(\n",
    "        epsilon=1.001e-5, name=\"batch_normalization_0\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Activation(activation, name=f\"{activation}_activation_0\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(\n",
    "        input_dim,\n",
    "        kernel_initializer=tf.keras.initializers.LecunUniform(),\n",
    "        name=\"predictor_output\",\n",
    "    )(x)\n",
    "    o = tfsim.layers.ActivationStdLoggingLayer(name=\"pred_std\")(x)\n",
    "    predictor = tf.keras.Model(inputs, o, name=\"predictor\")\n",
    "    return predictor\n",
    "\n",
    "\n",
    "predictor = get_predictor(input_dim=DIM, hidden_dim=512)\n",
    "predictor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self-Supervised Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGORITHM == \"simsiam\":\n",
    "    loss = tfsim.losses.SimSiamLoss(projection_type=\"cosine_distance\", name=ALGORITHM)\n",
    "\n",
    "    contrastive_model = tfsim.models.ContrastiveModel(\n",
    "        backbone=backbone,\n",
    "        projector=projector,\n",
    "        predictor=predictor,  # NOTE: simiam requires predictor model.\n",
    "        algorithm=ALGORITHM,\n",
    "        name=ALGORITHM,\n",
    "    )\n",
    "    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=INIT_LR,\n",
    "        decay_steps=PRE_TRAIN_EPOCHS * PRE_TRAIN_STEPS_PER_EPOCH,\n",
    "    )\n",
    "    wd_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=WEIGHT_DECAY,\n",
    "        decay_steps=PRE_TRAIN_EPOCHS * PRE_TRAIN_STEPS_PER_EPOCH,\n",
    "    )\n",
    "    optimizer = tfa.optimizers.SGDW(\n",
    "        learning_rate=lr_decayed_fn, weight_decay=wd_decayed_fn, momentum=0.9\n",
    "    )\n",
    "elif ALGORITHM == \"barlow\":\n",
    "    loss = tfsim.losses.Barlow(name=ALGORITHM)\n",
    "\n",
    "    contrastive_model = tfsim.models.ContrastiveModel(\n",
    "        backbone=backbone,\n",
    "        projector=projector,\n",
    "        algorithm=\"barlow\",\n",
    "    )\n",
    "    lr_decayed_fn = tfsim.schedules.WarmUpCosine(\n",
    "        initial_learning_rate=INIT_LR,\n",
    "        decay_steps=PRE_TRAIN_EPOCHS * PRE_TRAIN_STEPS_PER_EPOCH,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        warmup_learning_rate=WARMUP_LR,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9)\n",
    "elif ALGORITHM == \"simclr\":\n",
    "    loss = tfsim.losses.SimCLRLoss(name=ALGORITHM)\n",
    "\n",
    "    contrastive_model = tfsim.models.ContrastiveModel(\n",
    "        backbone=backbone,\n",
    "        projector=projector,\n",
    "        algorithm=ALGORITHM,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(INIT_LR)\n",
    "else:\n",
    "    raise ValueError(f\"{ALGORITHM} is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = data_path / \"models_owen\" / \"logs\" / f\"{loss.name}_{time.time()}\"\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"loss\", patience=50, restore_best_weights=True\n",
    "# )\n",
    "tbc = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    update_freq=100,\n",
    ")\n",
    "tsc = tfsim.callbacks.EvalCallback(\n",
    "    tf.cast(x_query, tf.float32),\n",
    "    y_query,\n",
    "    tf.cast(x_index, tf.float32),\n",
    "    y_index,\n",
    "    metrics=[\"binary_accuracy\"],\n",
    "    k=1,\n",
    "    tb_logdir=log_dir,\n",
    ")\n",
    "mcp_train = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=data_path / \"models_owen\" / \"checkpoints\" / f\"{loss.name}_{time.time()}\",\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train\n",
    "\n",
    "**Notes**\n",
    "- SGDW seems to be required to prevent the loss from becoming unstable.\n",
    "- Per layer kernel_regularization with weight decay doesn't work.\n",
    "- The kernel initialization seems very critical. Smaller, uniform initalization schemes like Lecunn seem to work best.\n",
    "- The projector is a more stable output for the KNN match metrics. The predictor ouput is typically worse and higher variance.\n",
    "- The LR and weight decay schedule seem important. The loss becomes unstable if the updates are too large later in training.\n",
    "- Too much capacity in the combined model and the training won't converge... same goes for too little capacity.\n",
    "- Augmentation is critical, but too much augmentation and the model won't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy: 0.1995\n",
      "87/87 [==============================] - 46s 525ms/step - loss: 0.2823 - proj_std: 0.0182 - pred_std: 0.0157 - val_loss: 0.1626 - val_proj_std: 0.0128 - val_pred_std: 0.0109 - binary_accuracy: 0.1995\n",
      "Epoch 4/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2278 - proj_std: 0.0177 - pred_std: 0.0162binary_accuracy: 0.1880\n",
      "87/87 [==============================] - 46s 528ms/step - loss: 0.2278 - proj_std: 0.0177 - pred_std: 0.0162 - val_loss: 0.1882 - val_proj_std: 0.0150 - val_pred_std: 0.0133 - binary_accuracy: 0.1880\n",
      "Epoch 5/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2011 - proj_std: 0.0176 - pred_std: 0.0162binary_accuracy: 0.2045\n",
      "87/87 [==============================] - 46s 524ms/step - loss: 0.2011 - proj_std: 0.0176 - pred_std: 0.0162 - val_loss: 0.0971 - val_proj_std: 0.0119 - val_pred_std: 0.0102 - binary_accuracy: 0.2045\n",
      "Epoch 6/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1912 - proj_std: 0.0178 - pred_std: 0.0165binary_accuracy: 0.1870\n",
      "87/87 [==============================] - 46s 528ms/step - loss: 0.1912 - proj_std: 0.0178 - pred_std: 0.0165 - val_loss: 0.1077 - val_proj_std: 0.0131 - val_pred_std: 0.0116 - binary_accuracy: 0.1870\n",
      "Epoch 7/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1887 - proj_std: 0.0179 - pred_std: 0.0166binary_accuracy: 0.1860\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1887 - proj_std: 0.0179 - pred_std: 0.0166 - val_loss: 0.0771 - val_proj_std: 0.0110 - val_pred_std: 0.0094 - binary_accuracy: 0.1860\n",
      "Epoch 8/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1943 - proj_std: 0.0182 - pred_std: 0.0170binary_accuracy: 0.2005\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1943 - proj_std: 0.0182 - pred_std: 0.0170 - val_loss: 0.1132 - val_proj_std: 0.0129 - val_pred_std: 0.0114 - binary_accuracy: 0.2005\n",
      "Epoch 9/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2333 - proj_std: 0.0184 - pred_std: 0.0170binary_accuracy: 0.1920\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.2333 - proj_std: 0.0184 - pred_std: 0.0170 - val_loss: 0.1931 - val_proj_std: 0.0136 - val_pred_std: 0.0117 - binary_accuracy: 0.1920\n",
      "Epoch 10/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2008 - proj_std: 0.0178 - pred_std: 0.0167binary_accuracy: 0.1755\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.2008 - proj_std: 0.0178 - pred_std: 0.0167 - val_loss: 0.1215 - val_proj_std: 0.0133 - val_pred_std: 0.0120 - binary_accuracy: 0.1755\n",
      "Epoch 11/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1849 - proj_std: 0.0177 - pred_std: 0.0166binary_accuracy: 0.1880\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1849 - proj_std: 0.0177 - pred_std: 0.0166 - val_loss: 0.0779 - val_proj_std: 0.0112 - val_pred_std: 0.0099 - binary_accuracy: 0.1880\n",
      "Epoch 12/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1883 - proj_std: 0.0182 - pred_std: 0.0171binary_accuracy: 0.1935\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1883 - proj_std: 0.0182 - pred_std: 0.0171 - val_loss: 0.0941 - val_proj_std: 0.0126 - val_pred_std: 0.0113 - binary_accuracy: 0.1935\n",
      "Epoch 13/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1865 - proj_std: 0.0185 - pred_std: 0.0174binary_accuracy: 0.1970\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1865 - proj_std: 0.0185 - pred_std: 0.0174 - val_loss: 0.0608 - val_proj_std: 0.0096 - val_pred_std: 0.0081 - binary_accuracy: 0.1970\n",
      "Epoch 14/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1855 - proj_std: 0.0187 - pred_std: 0.0178binary_accuracy: 0.2015\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1855 - proj_std: 0.0187 - pred_std: 0.0178 - val_loss: 0.0927 - val_proj_std: 0.0118 - val_pred_std: 0.0102 - binary_accuracy: 0.2015\n",
      "Epoch 15/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1862 - proj_std: 0.0189 - pred_std: 0.0181binary_accuracy: 0.1955\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1862 - proj_std: 0.0189 - pred_std: 0.0181 - val_loss: 0.0893 - val_proj_std: 0.0121 - val_pred_std: 0.0105 - binary_accuracy: 0.1955\n",
      "Epoch 16/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1945 - proj_std: 0.0192 - pred_std: 0.0184binary_accuracy: 0.2255\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1945 - proj_std: 0.0192 - pred_std: 0.0184 - val_loss: 0.0717 - val_proj_std: 0.0104 - val_pred_std: 0.0090 - binary_accuracy: 0.2255\n",
      "Epoch 17/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2010 - proj_std: 0.0193 - pred_std: 0.0186binary_accuracy: 0.2225\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.2010 - proj_std: 0.0193 - pred_std: 0.0186 - val_loss: 0.1082 - val_proj_std: 0.0130 - val_pred_std: 0.0115 - binary_accuracy: 0.2225\n",
      "Epoch 18/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1981 - proj_std: 0.0193 - pred_std: 0.0186binary_accuracy: 0.2575\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1981 - proj_std: 0.0193 - pred_std: 0.0186 - val_loss: 0.1044 - val_proj_std: 0.0113 - val_pred_std: 0.0099 - binary_accuracy: 0.2575\n",
      "Epoch 19/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1959 - proj_std: 0.0193 - pred_std: 0.0186binary_accuracy: 0.2500\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1959 - proj_std: 0.0193 - pred_std: 0.0186 - val_loss: 0.1066 - val_proj_std: 0.0127 - val_pred_std: 0.0111 - binary_accuracy: 0.2500\n",
      "Epoch 20/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1901 - proj_std: 0.0195 - pred_std: 0.0189binary_accuracy: 0.2725\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1901 - proj_std: 0.0195 - pred_std: 0.0189 - val_loss: 0.0943 - val_proj_std: 0.0125 - val_pred_std: 0.0110 - binary_accuracy: 0.2725\n",
      "Epoch 21/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1842 - proj_std: 0.0197 - pred_std: 0.0191binary_accuracy: 0.2835\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1842 - proj_std: 0.0197 - pred_std: 0.0191 - val_loss: 0.0902 - val_proj_std: 0.0125 - val_pred_std: 0.0112 - binary_accuracy: 0.2835\n",
      "Epoch 22/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1838 - proj_std: 0.0198 - pred_std: 0.0192binary_accuracy: 0.2730\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1838 - proj_std: 0.0198 - pred_std: 0.0192 - val_loss: 0.0966 - val_proj_std: 0.0135 - val_pred_std: 0.0123 - binary_accuracy: 0.2730\n",
      "Epoch 23/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1821 - proj_std: 0.0199 - pred_std: 0.0194binary_accuracy: 0.2715\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1821 - proj_std: 0.0199 - pred_std: 0.0194 - val_loss: 0.1056 - val_proj_std: 0.0139 - val_pred_std: 0.0124 - binary_accuracy: 0.2715\n",
      "Epoch 24/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1829 - proj_std: 0.0201 - pred_std: 0.0196binary_accuracy: 0.3000\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1829 - proj_std: 0.0201 - pred_std: 0.0196 - val_loss: 0.0835 - val_proj_std: 0.0122 - val_pred_std: 0.0111 - binary_accuracy: 0.3000\n",
      "Epoch 25/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1852 - proj_std: 0.0202 - pred_std: 0.0198binary_accuracy: 0.2945\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1852 - proj_std: 0.0202 - pred_std: 0.0198 - val_loss: 0.1007 - val_proj_std: 0.0138 - val_pred_std: 0.0126 - binary_accuracy: 0.2945\n",
      "Epoch 26/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1887 - proj_std: 0.0202 - pred_std: 0.0198binary_accuracy: 0.2960\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1887 - proj_std: 0.0202 - pred_std: 0.0198 - val_loss: 0.0975 - val_proj_std: 0.0137 - val_pred_std: 0.0125 - binary_accuracy: 0.2960\n",
      "Epoch 27/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1932 - proj_std: 0.0202 - pred_std: 0.0197binary_accuracy: 0.2830\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1932 - proj_std: 0.0202 - pred_std: 0.0197 - val_loss: 0.1200 - val_proj_std: 0.0143 - val_pred_std: 0.0133 - binary_accuracy: 0.2830\n",
      "Epoch 28/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1937 - proj_std: 0.0200 - pred_std: 0.0195binary_accuracy: 0.3065\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1937 - proj_std: 0.0200 - pred_std: 0.0195 - val_loss: 0.1067 - val_proj_std: 0.0137 - val_pred_std: 0.0126 - binary_accuracy: 0.3065\n",
      "Epoch 29/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2005 - proj_std: 0.0200 - pred_std: 0.0195binary_accuracy: 0.3015\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.2005 - proj_std: 0.0200 - pred_std: 0.0195 - val_loss: 0.1198 - val_proj_std: 0.0132 - val_pred_std: 0.0121 - binary_accuracy: 0.3015\n",
      "Epoch 30/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2022 - proj_std: 0.0199 - pred_std: 0.0194binary_accuracy: 0.3130\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.2022 - proj_std: 0.0199 - pred_std: 0.0194 - val_loss: 0.1304 - val_proj_std: 0.0138 - val_pred_std: 0.0123 - binary_accuracy: 0.3130\n",
      "Epoch 31/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2034 - proj_std: 0.0198 - pred_std: 0.0193binary_accuracy: 0.3225\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.2034 - proj_std: 0.0198 - pred_std: 0.0193 - val_loss: 0.1339 - val_proj_std: 0.0133 - val_pred_std: 0.0117 - binary_accuracy: 0.3225\n",
      "Epoch 32/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.2032 - proj_std: 0.0196 - pred_std: 0.0191binary_accuracy: 0.3045\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.2032 - proj_std: 0.0196 - pred_std: 0.0191 - val_loss: 0.1689 - val_proj_std: 0.0160 - val_pred_std: 0.0150 - binary_accuracy: 0.3045\n",
      "Epoch 33/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1971 - proj_std: 0.0194 - pred_std: 0.0189binary_accuracy: 0.3095\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1971 - proj_std: 0.0194 - pred_std: 0.0189 - val_loss: 0.1429 - val_proj_std: 0.0138 - val_pred_std: 0.0126 - binary_accuracy: 0.3095\n",
      "Epoch 34/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1909 - proj_std: 0.0195 - pred_std: 0.0190binary_accuracy: 0.3305\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1909 - proj_std: 0.0195 - pred_std: 0.0190 - val_loss: 0.1178 - val_proj_std: 0.0124 - val_pred_std: 0.0114 - binary_accuracy: 0.3305\n",
      "Epoch 35/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1901 - proj_std: 0.0194 - pred_std: 0.0189binary_accuracy: 0.3470\n",
      "87/87 [==============================] - 45s 522ms/step - loss: 0.1901 - proj_std: 0.0194 - pred_std: 0.0189 - val_loss: 0.1375 - val_proj_std: 0.0139 - val_pred_std: 0.0129 - binary_accuracy: 0.3470\n",
      "Epoch 36/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1920 - proj_std: 0.0194 - pred_std: 0.0190binary_accuracy: 0.3290\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1920 - proj_std: 0.0194 - pred_std: 0.0190 - val_loss: 0.1281 - val_proj_std: 0.0131 - val_pred_std: 0.0120 - binary_accuracy: 0.3290\n",
      "Epoch 37/800\n",
      "27/87 [========>.....................] - ETA: 27s - loss: 0.1870 - proj_std: 0.0191 - pred_std: 0.0188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 0.1754 - proj_std: 0.0189 - pred_std: 0.0185binary_accuracy: 0.3485\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1754 - proj_std: 0.0189 - pred_std: 0.0185 - val_loss: 0.2135 - val_proj_std: 0.0175 - val_pred_std: 0.0172 - binary_accuracy: 0.3485\n",
      "Epoch 45/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1675 - proj_std: 0.0191 - pred_std: 0.0187binary_accuracy: 0.3465\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1675 - proj_std: 0.0191 - pred_std: 0.0187 - val_loss: 0.1199 - val_proj_std: 0.0125 - val_pred_std: 0.0113 - binary_accuracy: 0.3465\n",
      "Epoch 46/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1608 - proj_std: 0.0191 - pred_std: 0.0187binary_accuracy: 0.3625\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1608 - proj_std: 0.0191 - pred_std: 0.0187 - val_loss: 0.1003 - val_proj_std: 0.0114 - val_pred_std: 0.0105 - binary_accuracy: 0.3625\n",
      "Epoch 47/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1576 - proj_std: 0.0191 - pred_std: 0.0188binary_accuracy: 0.3705\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1576 - proj_std: 0.0191 - pred_std: 0.0188 - val_loss: 0.0942 - val_proj_std: 0.0123 - val_pred_std: 0.0114 - binary_accuracy: 0.3705\n",
      "Epoch 48/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1578 - proj_std: 0.0193 - pred_std: 0.0190binary_accuracy: 0.3770\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1578 - proj_std: 0.0193 - pred_std: 0.0190 - val_loss: 0.1079 - val_proj_std: 0.0136 - val_pred_std: 0.0127 - binary_accuracy: 0.3770\n",
      "Epoch 49/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1559 - proj_std: 0.0196 - pred_std: 0.0193binary_accuracy: 0.3930\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1559 - proj_std: 0.0196 - pred_std: 0.0193 - val_loss: 0.1098 - val_proj_std: 0.0135 - val_pred_std: 0.0124 - binary_accuracy: 0.3930\n",
      "Epoch 50/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1634 - proj_std: 0.0197 - pred_std: 0.0194binary_accuracy: 0.3870\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1634 - proj_std: 0.0197 - pred_std: 0.0194 - val_loss: 0.2149 - val_proj_std: 0.0199 - val_pred_std: 0.0201 - binary_accuracy: 0.3870\n",
      "Epoch 51/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1590 - proj_std: 0.0200 - pred_std: 0.0196binary_accuracy: 0.3855\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1590 - proj_std: 0.0200 - pred_std: 0.0196 - val_loss: 0.1390 - val_proj_std: 0.0156 - val_pred_std: 0.0149 - binary_accuracy: 0.3855\n",
      "Epoch 52/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1555 - proj_std: 0.0204 - pred_std: 0.0201binary_accuracy: 0.4255\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1555 - proj_std: 0.0204 - pred_std: 0.0201 - val_loss: 0.1183 - val_proj_std: 0.0157 - val_pred_std: 0.0148 - binary_accuracy: 0.4255\n",
      "Epoch 53/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1544 - proj_std: 0.0204 - pred_std: 0.0201binary_accuracy: 0.3985\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1544 - proj_std: 0.0204 - pred_std: 0.0201 - val_loss: 0.1291 - val_proj_std: 0.0165 - val_pred_std: 0.0158 - binary_accuracy: 0.3985\n",
      "Epoch 54/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1519 - proj_std: 0.0205 - pred_std: 0.0202binary_accuracy: 0.4295\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1519 - proj_std: 0.0205 - pred_std: 0.0202 - val_loss: 0.1137 - val_proj_std: 0.0158 - val_pred_std: 0.0150 - binary_accuracy: 0.4295\n",
      "Epoch 55/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1553 - proj_std: 0.0206 - pred_std: 0.0204binary_accuracy: 0.4295\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1553 - proj_std: 0.0206 - pred_std: 0.0204 - val_loss: 0.1130 - val_proj_std: 0.0154 - val_pred_std: 0.0147 - binary_accuracy: 0.4295\n",
      "Epoch 56/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1559 - proj_std: 0.0207 - pred_std: 0.0205binary_accuracy: 0.4395\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.1559 - proj_std: 0.0207 - pred_std: 0.0205 - val_loss: 0.1483 - val_proj_std: 0.0167 - val_pred_std: 0.0163 - binary_accuracy: 0.4395\n",
      "Epoch 57/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1532 - proj_std: 0.0209 - pred_std: 0.0206binary_accuracy: 0.4415\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1532 - proj_std: 0.0209 - pred_std: 0.0206 - val_loss: 0.1141 - val_proj_std: 0.0155 - val_pred_std: 0.0149 - binary_accuracy: 0.4415\n",
      "Epoch 58/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1522 - proj_std: 0.0212 - pred_std: 0.0210binary_accuracy: 0.4535\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1522 - proj_std: 0.0212 - pred_std: 0.0210 - val_loss: 0.1382 - val_proj_std: 0.0191 - val_pred_std: 0.0188 - binary_accuracy: 0.4535\n",
      "Epoch 59/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1542 - proj_std: 0.0212 - pred_std: 0.0210binary_accuracy: 0.4615\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1542 - proj_std: 0.0212 - pred_std: 0.0210 - val_loss: 0.1463 - val_proj_std: 0.0182 - val_pred_std: 0.0181 - binary_accuracy: 0.4615\n",
      "Epoch 60/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1545 - proj_std: 0.0214 - pred_std: 0.0212binary_accuracy: 0.4770\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1545 - proj_std: 0.0214 - pred_std: 0.0212 - val_loss: 0.1174 - val_proj_std: 0.0183 - val_pred_std: 0.0179 - binary_accuracy: 0.4770\n",
      "Epoch 61/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1478 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.4850\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1478 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1268 - val_proj_std: 0.0190 - val_pred_std: 0.0186 - binary_accuracy: 0.4850\n",
      "Epoch 62/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1478 - proj_std: 0.0216 - pred_std: 0.0214binary_accuracy: 0.5065\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1478 - proj_std: 0.0216 - pred_std: 0.0214 - val_loss: 0.1357 - val_proj_std: 0.0194 - val_pred_std: 0.0193 - binary_accuracy: 0.5065\n",
      "Epoch 63/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1489 - proj_std: 0.0217 - pred_std: 0.0215binary_accuracy: 0.4985\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1489 - proj_std: 0.0217 - pred_std: 0.0215 - val_loss: 0.1442 - val_proj_std: 0.0194 - val_pred_std: 0.0192 - binary_accuracy: 0.4985\n",
      "Epoch 64/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1461 - proj_std: 0.0219 - pred_std: 0.0218binary_accuracy: 0.4975\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1461 - proj_std: 0.0219 - pred_std: 0.0218 - val_loss: 0.1550 - val_proj_std: 0.0204 - val_pred_std: 0.0206 - binary_accuracy: 0.4975\n",
      "Epoch 68/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1489 - proj_std: 0.0220 - pred_std: 0.0219binary_accuracy: 0.5270\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1489 - proj_std: 0.0220 - pred_std: 0.0219 - val_loss: 0.1459 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.5270\n",
      "Epoch 69/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1457 - proj_std: 0.0220 - pred_std: 0.0219binary_accuracy: 0.5125\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1457 - proj_std: 0.0220 - pred_std: 0.0219 - val_loss: 0.1449 - val_proj_std: 0.0206 - val_pred_std: 0.0205 - binary_accuracy: 0.5125\n",
      "Epoch 70/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1483 - proj_std: 0.0220 - pred_std: 0.0219binary_accuracy: 0.4990\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1483 - proj_std: 0.0220 - pred_std: 0.0219 - val_loss: 0.1379 - val_proj_std: 0.0200 - val_pred_std: 0.0201 - binary_accuracy: 0.4990\n",
      "Epoch 71/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1470 - proj_std: 0.0220 - pred_std: 0.0219binary_accuracy: 0.5240\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1470 - proj_std: 0.0220 - pred_std: 0.0219 - val_loss: 0.1571 - val_proj_std: 0.0206 - val_pred_std: 0.0207 - binary_accuracy: 0.5240\n",
      "Epoch 72/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1454 - proj_std: 0.0220 - pred_std: 0.0219binary_accuracy: 0.5290\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1454 - proj_std: 0.0220 - pred_std: 0.0219 - val_loss: 0.1516 - val_proj_std: 0.0214 - val_pred_std: 0.0213 - binary_accuracy: 0.5290\n",
      "Epoch 73/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1449 - proj_std: 0.0220 - pred_std: 0.0219binary_accuracy: 0.5030\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1449 - proj_std: 0.0220 - pred_std: 0.0219 - val_loss: 0.1405 - val_proj_std: 0.0205 - val_pred_std: 0.0207 - binary_accuracy: 0.5030\n",
      "Epoch 74/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1453 - proj_std: 0.0220 - pred_std: 0.0218binary_accuracy: 0.5210\n",
      "87/87 [==============================] - 45s 522ms/step - loss: 0.1453 - proj_std: 0.0220 - pred_std: 0.0218 - val_loss: 0.1485 - val_proj_std: 0.0205 - val_pred_std: 0.0204 - binary_accuracy: 0.5210\n",
      "Epoch 75/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1455 - proj_std: 0.0219 - pred_std: 0.0218binary_accuracy: 0.5470\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1455 - proj_std: 0.0219 - pred_std: 0.0218 - val_loss: 0.1428 - val_proj_std: 0.0208 - val_pred_std: 0.0210 - binary_accuracy: 0.5470\n",
      "Epoch 76/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1461 - proj_std: 0.0219 - pred_std: 0.0218binary_accuracy: 0.5145\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1461 - proj_std: 0.0219 - pred_std: 0.0218 - val_loss: 0.1439 - val_proj_std: 0.0211 - val_pred_std: 0.0212 - binary_accuracy: 0.5145\n",
      "Epoch 77/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1461 - proj_std: 0.0219 - pred_std: 0.0218binary_accuracy: 0.5300\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1461 - proj_std: 0.0219 - pred_std: 0.0218 - val_loss: 0.1499 - val_proj_std: 0.0206 - val_pred_std: 0.0206 - binary_accuracy: 0.5300\n",
      "Epoch 78/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1433 - proj_std: 0.0219 - pred_std: 0.0218binary_accuracy: 0.5410\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1433 - proj_std: 0.0219 - pred_std: 0.0218 - val_loss: 0.1476 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.5410\n",
      "Epoch 79/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1457 - proj_std: 0.0219 - pred_std: 0.0217binary_accuracy: 0.5470\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1457 - proj_std: 0.0219 - pred_std: 0.0217 - val_loss: 0.1552 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.5470\n",
      "Epoch 80/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1417 - proj_std: 0.0219 - pred_std: 0.0218binary_accuracy: 0.5525\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1417 - proj_std: 0.0219 - pred_std: 0.0218 - val_loss: 0.1325 - val_proj_std: 0.0204 - val_pred_std: 0.0205 - binary_accuracy: 0.5525\n",
      "Epoch 81/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1387 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5635\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1387 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1360 - val_proj_std: 0.0214 - val_pred_std: 0.0213 - binary_accuracy: 0.5635\n",
      "Epoch 82/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1413 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5820\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1413 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1369 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.5820\n",
      "Epoch 83/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1441 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5730\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1441 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1381 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.5730\n",
      "Epoch 84/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1423 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5520\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1423 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1651 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.5520\n",
      "Epoch 85/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1442 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5585\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1442 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1557 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.5585\n",
      "Epoch 86/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1437 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5215\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1437 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1467 - val_proj_std: 0.0205 - val_pred_std: 0.0205 - binary_accuracy: 0.5215\n",
      "Epoch 87/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1371 - proj_std: 0.0218 - pred_std: 0.0216binary_accuracy: 0.5720\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1371 - proj_std: 0.0218 - pred_std: 0.0216 - val_loss: 0.1322 - val_proj_std: 0.0203 - val_pred_std: 0.0202 - binary_accuracy: 0.5720\n",
      "Epoch 88/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1417 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5655\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1417 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1482 - val_proj_std: 0.0208 - val_pred_std: 0.0209 - binary_accuracy: 0.5655\n",
      "Epoch 89/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1393 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5815\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1393 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1595 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.5815\n",
      "Epoch 90/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1409 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5610\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1409 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1358 - val_proj_std: 0.0202 - val_pred_std: 0.0199 - binary_accuracy: 0.5610\n",
      "Epoch 91/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1379 - proj_std: 0.0218 - pred_std: 0.0216binary_accuracy: 0.5800\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1379 - proj_std: 0.0218 - pred_std: 0.0216 - val_loss: 0.1401 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.5800\n",
      "Epoch 92/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1416 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.5885\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1416 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1582 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.5885\n",
      "Epoch 93/800\n",
      " 4/87 [>.............................] - ETA: 37s - loss: 0.1447 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6265\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1311 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1528 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6265\n",
      "Epoch 149/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1312 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6105\n",
      "87/87 [==============================] - 45s 522ms/step - loss: 0.1312 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1387 - val_proj_std: 0.0213 - val_pred_std: 0.0212 - binary_accuracy: 0.6105\n",
      "Epoch 150/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1324 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6250\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1324 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1285 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.6250\n",
      "Epoch 151/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1312 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6420\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1312 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1580 - val_proj_std: 0.0213 - val_pred_std: 0.0213 - binary_accuracy: 0.6420\n",
      "Epoch 152/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1338 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6085\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1338 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1301 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.6085\n",
      "Epoch 153/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1274 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6445\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1274 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1319 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.6445\n",
      "Epoch 154/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1333 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6030\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1333 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1473 - val_proj_std: 0.0202 - val_pred_std: 0.0202 - binary_accuracy: 0.6030\n",
      "Epoch 155/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1338 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6335\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1338 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1538 - val_proj_std: 0.0202 - val_pred_std: 0.0202 - binary_accuracy: 0.6335\n",
      "Epoch 156/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1298 - proj_std: 0.0218 - pred_std: 0.0217binary_accuracy: 0.6240\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1298 - proj_std: 0.0218 - pred_std: 0.0217 - val_loss: 0.1514 - val_proj_std: 0.0210 - val_pred_std: 0.0211 - binary_accuracy: 0.6240\n",
      "Epoch 157/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1307 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6440\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1307 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1413 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6440\n",
      "Epoch 158/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1306 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6110\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1306 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1412 - val_proj_std: 0.0202 - val_pred_std: 0.0203 - binary_accuracy: 0.6110\n",
      "Epoch 159/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1312 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6425\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1312 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1462 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6425\n",
      "Epoch 160/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1338 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6145\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1338 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1294 - val_proj_std: 0.0187 - val_pred_std: 0.0191 - binary_accuracy: 0.6145\n",
      "Epoch 161/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1302 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6265\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1302 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1783 - val_proj_std: 0.0209 - val_pred_std: 0.0210 - binary_accuracy: 0.6265\n",
      "Epoch 162/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1290 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6220\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1290 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1709 - val_proj_std: 0.0206 - val_pred_std: 0.0207 - binary_accuracy: 0.6220\n",
      "Epoch 163/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1309 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6565\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1309 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1480 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.6565\n",
      "Epoch 164/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1289 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6330\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1289 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1374 - val_proj_std: 0.0208 - val_pred_std: 0.0209 - binary_accuracy: 0.6330\n",
      "Epoch 165/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1313 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6465\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1313 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1509 - val_proj_std: 0.0213 - val_pred_std: 0.0213 - binary_accuracy: 0.6465\n",
      "Epoch 166/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1321 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6560\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1321 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1371 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.6560\n",
      "Epoch 167/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1295 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6415\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1295 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1361 - val_proj_std: 0.0206 - val_pred_std: 0.0206 - binary_accuracy: 0.6415\n",
      "Epoch 168/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1298 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6565\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1298 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1398 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.6565\n",
      "Epoch 169/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1308 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6460\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1308 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1288 - val_proj_std: 0.0206 - val_pred_std: 0.0206 - binary_accuracy: 0.6460\n",
      "Epoch 170/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1319 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6295\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1319 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1441 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.6295\n",
      "Epoch 171/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1324 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6285\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1324 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1318 - val_proj_std: 0.0206 - val_pred_std: 0.0207 - binary_accuracy: 0.6285\n",
      "Epoch 172/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1306 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6040\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1306 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1528 - val_proj_std: 0.0210 - val_pred_std: 0.0211 - binary_accuracy: 0.6040\n",
      "Epoch 173/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1305 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6060\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1305 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1365 - val_proj_std: 0.0203 - val_pred_std: 0.0201 - binary_accuracy: 0.6060\n",
      "Epoch 174/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1298 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.6280\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1298 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1388 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.6280\n",
      "Epoch 175/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1291 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6400\n",
      "87/87 [==============================] - 45s 522ms/step - loss: 0.1291 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1534 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.6400\n",
      "Epoch 176/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1292 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6435\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1292 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1380 - val_proj_std: 0.0206 - val_pred_std: 0.0207 - binary_accuracy: 0.6435\n",
      "Epoch 177/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1294 - proj_std: 0.0216 - pred_std: 0.0215binary_accuracy: 0.6285\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1294 - proj_std: 0.0216 - pred_std: 0.0215 - val_loss: 0.1425 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.6285\n",
      "Epoch 178/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1318 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.6375\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1318 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1486 - val_proj_std: 0.0208 - val_pred_std: 0.0209 - binary_accuracy: 0.6375\n",
      "Epoch 179/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1320 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6095\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1320 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1401 - val_proj_std: 0.0194 - val_pred_std: 0.0194 - binary_accuracy: 0.6095\n",
      "Epoch 180/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1279 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6075\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1279 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1276 - val_proj_std: 0.0204 - val_pred_std: 0.0204 - binary_accuracy: 0.6075\n",
      "Epoch 181/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1252 - proj_std: 0.0216 - pred_std: 0.0215binary_accuracy: 0.6735\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1252 - proj_std: 0.0216 - pred_std: 0.0215 - val_loss: 0.1370 - val_proj_std: 0.0208 - val_pred_std: 0.0209 - binary_accuracy: 0.6735\n",
      "Epoch 182/800\n",
      "27/87 [========>.....................] - ETA: 27s - loss: 0.1299 - proj_std: 0.0217 - pred_std: 0.0216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy: 0.6555\n",
      "87/87 [==============================] - 46s 528ms/step - loss: 0.1221 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1476 - val_proj_std: 0.0207 - val_pred_std: 0.0206 - binary_accuracy: 0.6555\n",
      "Epoch 225/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1250 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6885\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1250 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1451 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.6885\n",
      "Epoch 230/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1258 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6805\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1258 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1430 - val_proj_std: 0.0213 - val_pred_std: 0.0212 - binary_accuracy: 0.6805\n",
      "Epoch 231/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1273 - proj_std: 0.0217 - pred_std: 0.0217binary_accuracy: 0.6620\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1273 - proj_std: 0.0217 - pred_std: 0.0217 - val_loss: 0.1403 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.6620\n",
      "Epoch 232/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1266 - proj_std: 0.0217 - pred_std: 0.0217binary_accuracy: 0.6640\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1266 - proj_std: 0.0217 - pred_std: 0.0217 - val_loss: 0.1460 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.6640\n",
      "Epoch 233/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1255 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6820\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1255 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1464 - val_proj_std: 0.0213 - val_pred_std: 0.0213 - binary_accuracy: 0.6820\n",
      "Epoch 234/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1233 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6835\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1233 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1653 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.6835\n",
      "Epoch 235/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1252 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6595\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1252 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1438 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.6595\n",
      "Epoch 236/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1242 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6535\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1242 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1548 - val_proj_std: 0.0207 - val_pred_std: 0.0204 - binary_accuracy: 0.6535\n",
      "Epoch 237/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1232 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6665\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1232 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1510 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.6665\n",
      "Epoch 238/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1250 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6935\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1250 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1378 - val_proj_std: 0.0212 - val_pred_std: 0.0211 - binary_accuracy: 0.6935\n",
      "Epoch 239/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1240 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6635\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1240 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1490 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6635\n",
      "Epoch 240/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1246 - proj_std: 0.0217 - pred_std: 0.0217binary_accuracy: 0.6690\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1246 - proj_std: 0.0217 - pred_std: 0.0217 - val_loss: 0.1431 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.6690\n",
      "Epoch 241/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1246 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6865\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1246 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1426 - val_proj_std: 0.0214 - val_pred_std: 0.0213 - binary_accuracy: 0.6865\n",
      "Epoch 242/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1251 - proj_std: 0.0217 - pred_std: 0.0217binary_accuracy: 0.6555\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1251 - proj_std: 0.0217 - pred_std: 0.0217 - val_loss: 0.1515 - val_proj_std: 0.0212 - val_pred_std: 0.0211 - binary_accuracy: 0.6555\n",
      "Epoch 243/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1236 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6775\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1236 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1357 - val_proj_std: 0.0213 - val_pred_std: 0.0212 - binary_accuracy: 0.6775\n",
      "Epoch 244/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1239 - proj_std: 0.0217 - pred_std: 0.0217binary_accuracy: 0.6535\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1239 - proj_std: 0.0217 - pred_std: 0.0217 - val_loss: 0.1507 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.6535\n",
      "Epoch 245/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1246 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6480\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1246 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1376 - val_proj_std: 0.0207 - val_pred_std: 0.0207 - binary_accuracy: 0.6480\n",
      "Epoch 246/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1244 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6635\n",
      "87/87 [==============================] - 45s 520ms/step - loss: 0.1244 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1445 - val_proj_std: 0.0208 - val_pred_std: 0.0207 - binary_accuracy: 0.6635\n",
      "Epoch 247/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1248 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6860\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1248 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1539 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6860\n",
      "Epoch 248/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1218 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.6950\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1218 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1357 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6950\n",
      "Epoch 249/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1218 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6555\n",
      "87/87 [==============================] - 45s 522ms/step - loss: 0.1218 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1350 - val_proj_std: 0.0207 - val_pred_std: 0.0207 - binary_accuracy: 0.6555\n",
      "Epoch 250/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1218 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6675\n",
      "87/87 [==============================] - 46s 528ms/step - loss: 0.1218 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1449 - val_proj_std: 0.0208 - val_pred_std: 0.0209 - binary_accuracy: 0.6675\n",
      "Epoch 251/800\n",
      "47/87 [===============>..............] - ETA: 18s - loss: 0.1233 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6905\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.1193 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1306 - val_proj_std: 0.0203 - val_pred_std: 0.0202 - binary_accuracy: 0.6905\n",
      "Epoch 310/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1174 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.6925\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1174 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1435 - val_proj_std: 0.0211 - val_pred_std: 0.0212 - binary_accuracy: 0.6925\n",
      "Epoch 311/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1184 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6995\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1184 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1436 - val_proj_std: 0.0207 - val_pred_std: 0.0206 - binary_accuracy: 0.6995\n",
      "Epoch 312/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1193 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7150\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1193 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1238 - val_proj_std: 0.0206 - val_pred_std: 0.0207 - binary_accuracy: 0.7150\n",
      "Epoch 313/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1182 - proj_std: 0.0216 - pred_std: 0.0215binary_accuracy: 0.6995\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1182 - proj_std: 0.0216 - pred_std: 0.0215 - val_loss: 0.1436 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.6995\n",
      "Epoch 314/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1188 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7070\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1188 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1340 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7070\n",
      "Epoch 315/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1180 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7205\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1180 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1517 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7205\n",
      "Epoch 316/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1171 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7195\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1171 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1313 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.7195\n",
      "Epoch 317/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1183 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7225\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1183 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1460 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7225\n",
      "Epoch 318/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1159 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7005\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1159 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1338 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7005\n",
      "Epoch 319/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1159 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7125\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1159 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1376 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7125\n",
      "Epoch 320/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1172 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6760\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1172 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1351 - val_proj_std: 0.0206 - val_pred_std: 0.0207 - binary_accuracy: 0.6760\n",
      "Epoch 321/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1181 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7145\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1181 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1254 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7145\n",
      "Epoch 322/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1179 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7240\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1179 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1248 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7240\n",
      "Epoch 323/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1160 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7210\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1160 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1395 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7210\n",
      "Epoch 324/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1171 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7140\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1171 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1297 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7140\n",
      "Epoch 325/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1175 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6850\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1175 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1374 - val_proj_std: 0.0208 - val_pred_std: 0.0209 - binary_accuracy: 0.6850\n",
      "Epoch 326/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1179 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7265\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1179 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1253 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7265\n",
      "Epoch 327/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1153 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6950\n",
      "87/87 [==============================] - 46s 530ms/step - loss: 0.1153 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1335 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6950\n",
      "Epoch 328/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1177 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7160\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1177 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1431 - val_proj_std: 0.0213 - val_pred_std: 0.0212 - binary_accuracy: 0.7160\n",
      "Epoch 329/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1167 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7055\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1167 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1474 - val_proj_std: 0.0208 - val_pred_std: 0.0207 - binary_accuracy: 0.7055\n",
      "Epoch 330/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1171 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6980\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1171 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1452 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.6980\n",
      "Epoch 331/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1180 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7045\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1180 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1499 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.7045\n",
      "Epoch 332/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1186 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6995\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1186 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1284 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.6995\n",
      "Epoch 333/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1179 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6995\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1179 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1429 - val_proj_std: 0.0205 - val_pred_std: 0.0204 - binary_accuracy: 0.6995\n",
      "Epoch 334/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1198 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.7195\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1198 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1406 - val_proj_std: 0.0214 - val_pred_std: 0.0213 - binary_accuracy: 0.7195\n",
      "Epoch 335/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1186 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6795\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1186 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1285 - val_proj_std: 0.0200 - val_pred_std: 0.0201 - binary_accuracy: 0.6795\n",
      "Epoch 336/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1175 - proj_std: 0.0217 - pred_std: 0.0216binary_accuracy: 0.6795\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1175 - proj_std: 0.0217 - pred_std: 0.0216 - val_loss: 0.1383 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.6795\n",
      "Epoch 337/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1176 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.6725\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1176 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1308 - val_proj_std: 0.0193 - val_pred_std: 0.0192 - binary_accuracy: 0.6725\n",
      "Epoch 338/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1190 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7035\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1190 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1318 - val_proj_std: 0.0199 - val_pred_std: 0.0199 - binary_accuracy: 0.7035\n",
      "Epoch 339/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1183 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.7060\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1183 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1551 - val_proj_std: 0.0206 - val_pred_std: 0.0205 - binary_accuracy: 0.7060\n",
      "Epoch 340/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1178 - proj_std: 0.0216 - pred_std: 0.0216binary_accuracy: 0.6775\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1178 - proj_std: 0.0216 - pred_std: 0.0216 - val_loss: 0.1239 - val_proj_std: 0.0192 - val_pred_std: 0.0190 - binary_accuracy: 0.6775\n",
      "Epoch 341/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1171 - proj_std: 0.0216 - pred_std: 0.0215binary_accuracy: 0.7145\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1171 - proj_std: 0.0216 - pred_std: 0.0215 - val_loss: 0.1343 - val_proj_std: 0.0210 - val_pred_std: 0.0211 - binary_accuracy: 0.7145\n",
      "Epoch 342/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1170 - proj_std: 0.0216 - pred_std: 0.0215binary_accuracy: 0.6895\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1170 - proj_std: 0.0216 - pred_std: 0.0215 - val_loss: 0.1392 - val_proj_std: 0.0205 - val_pred_std: 0.0206 - binary_accuracy: 0.6895\n",
      "Epoch 343/800\n",
      "27/87 [========>.....................] - ETA: 27s - loss: 0.1171 - proj_std: 0.0216 - pred_std: 0.0215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 0.1151 - proj_std: 0.0215 - pred_std: 0.0215binary_accuracy: 0.7355\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1151 - proj_std: 0.0215 - pred_std: 0.0215 - val_loss: 0.1312 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7355\n",
      "Epoch 387/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1142 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7195\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1142 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1267 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7195\n",
      "Epoch 391/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1101 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7415\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.1101 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1230 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7415\n",
      "Epoch 392/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1132 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7165\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1132 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1492 - val_proj_std: 0.0207 - val_pred_std: 0.0207 - binary_accuracy: 0.7165\n",
      "Epoch 393/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1073 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7255\n",
      "87/87 [==============================] - 46s 528ms/step - loss: 0.1073 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1312 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.7255\n",
      "Epoch 394/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1097 - proj_std: 0.0214 - pred_std: 0.0214binary_accuracy: 0.7355\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1097 - proj_std: 0.0214 - pred_std: 0.0214 - val_loss: 0.1320 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7355\n",
      "Epoch 395/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1092 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7330\n",
      "87/87 [==============================] - 45s 520ms/step - loss: 0.1092 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1343 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7330\n",
      "Epoch 396/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1095 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7090\n",
      "87/87 [==============================] - 45s 520ms/step - loss: 0.1095 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1261 - val_proj_std: 0.0205 - val_pred_std: 0.0205 - binary_accuracy: 0.7090\n",
      "Epoch 397/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1077 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7355\n",
      "87/87 [==============================] - 45s 520ms/step - loss: 0.1077 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1365 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7355\n",
      "Epoch 398/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1101 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7330\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1101 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1378 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7330\n",
      "Epoch 399/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1092 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7610\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1092 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1268 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7610\n",
      "Epoch 400/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1074 - proj_std: 0.0214 - pred_std: 0.0214binary_accuracy: 0.7405\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1074 - proj_std: 0.0214 - pred_std: 0.0214 - val_loss: 0.1415 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7405\n",
      "Epoch 401/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1104 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7365\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1104 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1342 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7365\n",
      "Epoch 402/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1046 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7410\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.1046 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1430 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.7410\n",
      "Epoch 403/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1092 - proj_std: 0.0214 - pred_std: 0.0214binary_accuracy: 0.7450\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1092 - proj_std: 0.0214 - pred_std: 0.0214 - val_loss: 0.1312 - val_proj_std: 0.0212 - val_pred_std: 0.0212 - binary_accuracy: 0.7450\n",
      "Epoch 404/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1034 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7265\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.1034 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1146 - val_proj_std: 0.0207 - val_pred_std: 0.0206 - binary_accuracy: 0.7265\n",
      "Epoch 405/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1075 - proj_std: 0.0213 - pred_std: 0.0213binary_accuracy: 0.7270\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1075 - proj_std: 0.0213 - pred_std: 0.0213 - val_loss: 0.1261 - val_proj_std: 0.0209 - val_pred_std: 0.0207 - binary_accuracy: 0.7270\n",
      "Epoch 406/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1108 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7550\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1108 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1295 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7550\n",
      "Epoch 407/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1081 - proj_std: 0.0214 - pred_std: 0.0213binary_accuracy: 0.7420\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1081 - proj_std: 0.0214 - pred_std: 0.0213 - val_loss: 0.1288 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7420\n",
      "Epoch 408/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1096 - proj_std: 0.0215 - pred_std: 0.0214binary_accuracy: 0.7315\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1096 - proj_std: 0.0215 - pred_std: 0.0214 - val_loss: 0.1397 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7315\n",
      "Epoch 409/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1021 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7625\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1021 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1315 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7625\n",
      "Epoch 473/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1031 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7720\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1031 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1264 - val_proj_std: 0.0211 - val_pred_std: 0.0210 - binary_accuracy: 0.7720\n",
      "Epoch 474/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1009 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7545\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1009 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1272 - val_proj_std: 0.0207 - val_pred_std: 0.0205 - binary_accuracy: 0.7545\n",
      "Epoch 475/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1011 - proj_std: 0.0212 - pred_std: 0.0212binary_accuracy: 0.7460\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1011 - proj_std: 0.0212 - pred_std: 0.0212 - val_loss: 0.1280 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.7460\n",
      "Epoch 476/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1050 - proj_std: 0.0213 - pred_std: 0.0213binary_accuracy: 0.7395\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.1050 - proj_std: 0.0213 - pred_std: 0.0213 - val_loss: 0.1317 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7395\n",
      "Epoch 477/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0986 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7625\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.0986 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1312 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7625\n",
      "Epoch 478/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1033 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7650\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1033 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1312 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7650\n",
      "Epoch 479/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1006 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7450\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1006 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1563 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.7450\n",
      "Epoch 480/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1033 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7545\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.1033 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1323 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7545\n",
      "Epoch 481/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1029 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7345\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1029 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1200 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.7345\n",
      "Epoch 482/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1001 - proj_std: 0.0212 - pred_std: 0.0212binary_accuracy: 0.7615\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1001 - proj_std: 0.0212 - pred_std: 0.0212 - val_loss: 0.1334 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7615\n",
      "Epoch 483/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1036 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7385\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1036 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1231 - val_proj_std: 0.0208 - val_pred_std: 0.0207 - binary_accuracy: 0.7385\n",
      "Epoch 484/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1007 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7610\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1007 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1408 - val_proj_std: 0.0209 - val_pred_std: 0.0210 - binary_accuracy: 0.7610\n",
      "Epoch 485/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0976 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7495\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.0976 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1245 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.7495\n",
      "Epoch 486/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1022 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7575\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1022 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1482 - val_proj_std: 0.0209 - val_pred_std: 0.0210 - binary_accuracy: 0.7575\n",
      "Epoch 487/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1010 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7560\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1010 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1195 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.7560\n",
      "Epoch 488/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1025 - proj_std: 0.0213 - pred_std: 0.0213binary_accuracy: 0.7605\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1025 - proj_std: 0.0213 - pred_std: 0.0213 - val_loss: 0.1316 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7605\n",
      "Epoch 489/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1027 - proj_std: 0.0213 - pred_std: 0.0213binary_accuracy: 0.7535\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1027 - proj_std: 0.0213 - pred_std: 0.0213 - val_loss: 0.1371 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.7535\n",
      "Epoch 490/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0989 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7520\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0989 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1318 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7520\n",
      "Epoch 491/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1005 - proj_std: 0.0213 - pred_std: 0.0212binary_accuracy: 0.7420\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.1005 - proj_std: 0.0213 - pred_std: 0.0212 - val_loss: 0.1306 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7420\n",
      "Epoch 492/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0930 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7890\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0930 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1252 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.7890\n",
      "Epoch 554/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0897 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7885\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.0897 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1195 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7885\n",
      "Epoch 555/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0925 - proj_std: 0.0212 - pred_std: 0.0212binary_accuracy: 0.7970\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0925 - proj_std: 0.0212 - pred_std: 0.0212 - val_loss: 0.1231 - val_proj_std: 0.0209 - val_pred_std: 0.0208 - binary_accuracy: 0.7970\n",
      "Epoch 556/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0906 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7935\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0906 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1253 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7935\n",
      "Epoch 557/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0934 - proj_std: 0.0212 - pred_std: 0.0212binary_accuracy: 0.7870\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0934 - proj_std: 0.0212 - pred_std: 0.0212 - val_loss: 0.1185 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7870\n",
      "Epoch 558/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0885 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8005\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.0885 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1241 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.8005\n",
      "Epoch 559/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0914 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7965\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0914 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1148 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7965\n",
      "Epoch 560/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0926 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7900\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0926 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1298 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7900\n",
      "Epoch 561/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0931 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.8020\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0931 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1245 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.8020\n",
      "Epoch 562/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0905 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7975\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0905 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1301 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.7975\n",
      "Epoch 563/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0905 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7945\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0905 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1217 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7945\n",
      "Epoch 564/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0896 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7880\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0896 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1261 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7880\n",
      "Epoch 565/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0893 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8000\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.0893 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1312 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.8000\n",
      "Epoch 566/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0901 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7805\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0901 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1240 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7805\n",
      "Epoch 567/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0897 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8010\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0897 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1231 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.8010\n",
      "Epoch 568/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0901 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7935\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.0901 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1138 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7935\n",
      "Epoch 569/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0889 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7935\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.0889 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1266 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7935\n",
      "Epoch 570/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0880 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7830\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.0880 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1268 - val_proj_std: 0.0208 - val_pred_std: 0.0207 - binary_accuracy: 0.7830\n",
      "Epoch 571/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0892 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7900\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0892 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1263 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7900\n",
      "Epoch 572/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0894 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.8000\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0894 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1290 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.8000\n",
      "Epoch 573/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0907 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7805\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0907 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1244 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.7805\n",
      "Epoch 574/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0903 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7955\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0903 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1221 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7955\n",
      "Epoch 575/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0891 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7980\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0891 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1315 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7980\n",
      "Epoch 576/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0902 - proj_std: 0.0212 - pred_std: 0.0211binary_accuracy: 0.7950\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0902 - proj_std: 0.0212 - pred_std: 0.0211 - val_loss: 0.1210 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.7950\n",
      "Epoch 577/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0873 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7890\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.0873 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1189 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7890\n",
      "Epoch 578/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0886 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7775\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0886 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1160 - val_proj_std: 0.0208 - val_pred_std: 0.0208 - binary_accuracy: 0.7775\n",
      "Epoch 579/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0878 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7870\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0878 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1239 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7870\n",
      "Epoch 580/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0872 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8020\n",
      "87/87 [==============================] - 46s 527ms/step - loss: 0.0872 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1319 - val_proj_std: 0.0210 - val_pred_std: 0.0209 - binary_accuracy: 0.8020\n",
      "Epoch 581/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0887 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8035\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0887 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1241 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.8035\n",
      "Epoch 582/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0874 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7870\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0874 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1217 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7870\n",
      "Epoch 583/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0883 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7965\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0883 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1203 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.7965\n",
      "Epoch 584/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0884 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.7735\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0884 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1286 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.7735\n",
      "Epoch 585/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0877 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8055\n",
      "87/87 [==============================] - 45s 519ms/step - loss: 0.0877 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1280 - val_proj_std: 0.0210 - val_pred_std: 0.0210 - binary_accuracy: 0.8055\n",
      "Epoch 586/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0874 - proj_std: 0.0211 - pred_std: 0.0211binary_accuracy: 0.8035\n",
      "87/87 [==============================] - 45s 518ms/step - loss: 0.0874 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1206 - val_proj_std: 0.0211 - val_pred_std: 0.0211 - binary_accuracy: 0.8035\n",
      "Epoch 587/800\n",
      "22/87 [======>.......................] - ETA: 29s - loss: 0.0860 - proj_std: 0.0211 - pred_std: 0.0211"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 46s 528ms/step - loss: 0.0782 - proj_std: 0.0211 - pred_std: 0.0211 - val_loss: 0.1261 - val_proj_std: 0.0209 - val_pred_std: 0.0209 - binary_accuracy: 0.8220\n",
      "Epoch 629/800\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0788 - proj_std: 0.0211 - pred_std: 0.0211"
     ]
    }
   ],
   "source": [
    "history = contrastive_model.fit(\n",
    "    train_ds,\n",
    "    epochs=PRE_TRAIN_EPOCHS,\n",
    "    steps_per_epoch=PRE_TRAIN_STEPS_PER_EPOCH,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    "    callbacks=[tsc, tbc, mcp_train],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.grid()\n",
    "plt.title(f\"{loss.name} - loss\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(history.history[\"proj_std\"], label=\"proj\")\n",
    "plt.plot(history.history[\"pred_std\"], label=\"pred\")\n",
    "plt.grid()\n",
    "plt.title(f\"{loss.name} - std metrics\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(history.history[\"binary_accuracy\"], label=\"acc\")\n",
    "plt.grid()\n",
    "plt.title(f\"{loss.name} - match metrics\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.grid()\n",
    "plt.title(f\"{loss.name} - loss\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(history.history[\"proj_std\"], label=\"proj\")\n",
    "plt.plot(history.history[\"pred_std\"], label=\"pred\")\n",
    "plt.grid()\n",
    "plt.title(f\"{loss.name} - std metrics\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(history.history[\"binary_accuracy\"], label=\"acc\")\n",
    "plt.grid()\n",
    "plt.title(f\"{loss.name} - match metrics\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history[\"binary_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1c26eeae10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_model.load_weights(data_path / 'models_owen' / 'checkpoints' / 'simsiam_80acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[Saving backbone model]\u001b[0m\n",
      "\u001b[32m|-path:../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/backbone\u001b[0m\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2021-12-29 19:12:57.757048: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/backbone/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/backbone/assets\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index not saved as save_index=False\n",
      "\u001b[34m[Saving projector model]\u001b[0m\n",
      "\u001b[32m|-path:../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/projector\u001b[0m\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/projector/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/projector/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[Saving predictor model]\u001b[0m\n",
      "\u001b[32m|-path:../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/predictor\u001b[0m\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/predictor/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../../kaggle_ds/google_landmarks/models_owen/simsiam__85acc/predictor/assets\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "contrastive_model.save(data_path / 'models_owen' / f'simsiam__85acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del contrastive_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "contrastive_model = tfsim.models.contrastive_model.load_model(data_path / 'models_owen' / f'simsiam_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EPOCHS = 10\n",
    "TEST_STEPS_PER_EPOCH = int(len(x_train) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_augmenter(img):\n",
    "    # random resize and crop. Increase the size before we crop.\n",
    "    img = tfsim.augmenters.simclr.crop_and_resize(\n",
    "        img, CIFAR_IMG_SIZE, CIFAR_IMG_SIZE, area_range=(0.2, 1.0)\n",
    "    )\n",
    "    # random horizontal flip\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, tf.keras.utils.to_categorical(y_train, 10))\n",
    ")\n",
    "eval_train_ds = eval_train_ds.repeat()\n",
    "eval_train_ds = eval_train_ds.shuffle(1024)\n",
    "eval_train_ds = eval_train_ds.map(lambda x, y: (eval_augmenter(x), y), tf.data.AUTOTUNE)\n",
    "eval_train_ds = eval_train_ds.batch(BATCH_SIZE)\n",
    "eval_train_ds = eval_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_val, tf.keras.utils.to_categorical(y_val, 10))\n",
    ")\n",
    "eval_val_ds = eval_val_ds.repeat()\n",
    "eval_val_ds = eval_val_ds.shuffle(1024)\n",
    "eval_val_ds = eval_val_ds.map(lambda x, y: (x, y), tf.data.AUTOTUNE)\n",
    "eval_val_ds = eval_val_ds.batch(BATCH_SIZE)\n",
    "eval_val_ds = eval_val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test, tf.keras.utils.to_categorical(y_test, 10))\n",
    ")\n",
    "eval_test_ds = eval_test_ds.map(lambda x, y: (x, y), tf.data.AUTOTUNE)\n",
    "eval_test_ds = eval_test_ds.batch(BATCH_SIZE)\n",
    "eval_test_ds = eval_test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_model(img_size, backbone, total_steps, trainable=True, lr=1.8):\n",
    "    backbone.trainable = trainable\n",
    "    inputs = tf.keras.layers.Input((img_size, img_size, 3), name=\"eval_input\")\n",
    "    x = backbone(inputs, training=trainable)\n",
    "    o = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, o)\n",
    "    cosine_decayed_lr = tf.keras.experimental.CosineDecay(\n",
    "        initial_learning_rate=lr, decay_steps=total_steps\n",
    "    )\n",
    "    opt = tf.keras.optimizers.SGD(cosine_decayed_lr, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 22s 242ms/step - loss: 2.0133 - acc: 0.2575 - val_loss: 1.7595 - val_acc: 0.3441\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.7438 - acc: 0.3521 - val_loss: 1.6239 - val_acc: 0.3964\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.6523 - acc: 0.3894 - val_loss: 1.5555 - val_acc: 0.4359\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.6021 - acc: 0.4136 - val_loss: 1.5047 - val_acc: 0.4550\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.5634 - acc: 0.4296 - val_loss: 1.4722 - val_acc: 0.4733\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.5329 - acc: 0.4428 - val_loss: 1.4496 - val_acc: 0.4833\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.5144 - acc: 0.4483 - val_loss: 1.4334 - val_acc: 0.4895\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.4916 - acc: 0.4591 - val_loss: 1.4288 - val_acc: 0.4861\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.4945 - acc: 0.4606 - val_loss: 1.4280 - val_acc: 0.4868\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 1.4838 - acc: 0.4626 - val_loss: 1.4289 - val_acc: 0.4859\n"
     ]
    }
   ],
   "source": [
    "no_pt_eval_model = get_eval_model(\n",
    "    img_size=CIFAR_IMG_SIZE,\n",
    "    backbone=get_backbone(CIFAR_IMG_SIZE, DIM),\n",
    "    total_steps=TEST_EPOCHS * TEST_STEPS_PER_EPOCH,\n",
    "    trainable=True,\n",
    "    lr=1e-3,\n",
    ")\n",
    "no_pt_history = no_pt_eval_model.fit(\n",
    "    eval_train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=TEST_EPOCHS,\n",
    "    steps_per_epoch=TEST_STEPS_PER_EPOCH,\n",
    "    validation_data=eval_val_ds,\n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " eval_input (InputLayer)     [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " resnet18sim (SimilarityMode  (None, 512)              11182784  \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,187,914\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 11,182,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pt_eval_model = get_eval_model(\n",
    "    CIFAR_IMG_SIZE,\n",
    "    contrastive_model.backbone,\n",
    "    total_steps=TEST_EPOCHS * TEST_STEPS_PER_EPOCH,\n",
    "    trainable=False,\n",
    "    lr=30.0,\n",
    ")\n",
    "pt_eval_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 8s 74ms/step - loss: 41.8471 - acc: 0.1075 - val_loss: 50.4189 - val_acc: 0.1202\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 36.4956 - acc: 0.1303 - val_loss: 45.4352 - val_acc: 0.0999\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 20.4054 - acc: 0.2074 - val_loss: 11.2029 - val_acc: 0.3952\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 6s 70ms/step - loss: 3.5774 - acc: 0.5970 - val_loss: 0.4436 - val_acc: 0.8585\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 6s 70ms/step - loss: 0.4112 - acc: 0.8684 - val_loss: 0.4624 - val_acc: 0.8491\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 0.3922 - acc: 0.8726 - val_loss: 0.3989 - val_acc: 0.8650\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 0.3822 - acc: 0.8758 - val_loss: 0.3841 - val_acc: 0.8744\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 0.3766 - acc: 0.8772 - val_loss: 0.3781 - val_acc: 0.8746\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 0.3708 - acc: 0.8775 - val_loss: 0.3766 - val_acc: 0.8703\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 6s 71ms/step - loss: 0.3714 - acc: 0.8786 - val_loss: 0.3824 - val_acc: 0.8709\n"
     ]
    }
   ],
   "source": [
    "pt_history = pt_eval_model.fit(\n",
    "    eval_train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=TEST_EPOCHS,\n",
    "    steps_per_epoch=TEST_STEPS_PER_EPOCH,\n",
    "    validation_data=eval_val_ds,\n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 75ms/step - loss: 1.3867 - acc: 0.4966\n",
      "no pretrain [1.3866854906082153, 0.4966000020503998]\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 0.3918 - acc: 0.8697"
     ]
    }
   ],
   "source": [
    "# SimSiam\n",
    "print(\"no pretrain\", no_pt_eval_model.evaluate(eval_test_ds))\n",
    "print(\"pretrained\", pt_eval_model.evaluate(eval_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 34ms/step - loss: 1.4910 - acc: 0.6161\n",
      "no pretrain [1.491011142730713, 0.616100013256073]\n",
      "40/40 [==============================] - 1s 30ms/step - loss: 2.0020 - acc: 0.4137\n",
      "pretrained [2.002046823501587, 0.41370001435279846]\n"
     ]
    }
   ],
   "source": [
    "# SimCLR\n",
    "print(\"no pretrain\", no_pt_eval_model.evaluate(eval_test_ds))\n",
    "print(\"pretrained\", pt_eval_model.evaluate(eval_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 32ms/step - loss: nan - acc: 0.1000\n",
      "no pretrain [nan, 0.10000000149011612]\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 3.5400 - acc: 0.4260\n",
      "pretrained [3.540032386779785, 0.4259999990463257]\n"
     ]
    }
   ],
   "source": [
    "# Barlow Pre-train Cifar 10 Linear classifier on Cifar 100\n",
    "print(\"no pretrain\", no_pt_eval_model.evaluate(eval_test_ds))\n",
    "print(\"pretrained\", pt_eval_model.evaluate(eval_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 32ms/step - loss: 2.5204 - acc: 0.3977\n",
      "no pretrain [2.5204174518585205, 0.3977000117301941]\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 2.2891 - acc: 0.6451\n",
      "pretrained [2.2890686988830566, 0.6450999975204468]\n"
     ]
    }
   ],
   "source": [
    "# Barlow Pre-train Cifar 10 Linear classifier on Cifar 10\n",
    "print(\"no pretrain\", no_pt_eval_model.evaluate(eval_test_ds))\n",
    "print(\"pretrained\", pt_eval_model.evaluate(eval_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tfsim",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "interpreter": {
   "hash": "cada0c3e68bbe3038c05f3f13c34d05fb5411cc128b62a4a529880c33c3268c3"
  },
  "kernelspec": {
   "display_name": "TF Sim",
   "language": "python",
   "name": "tfsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
